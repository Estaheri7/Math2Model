{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) Implementation Using Pytorch\n",
    "\n",
    "This notebook demonstrates the implementation of a Vision Transformer (ViT) from scratch using PyTorch. The Vision Transformer is a model for image classification that leverages the Transformer architecture, which has been highly successful in natural language processing tasks.\n",
    "\n",
    "We will be using the CIFAR-10 dataset for training and evaluating our model. The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 test images.\n",
    "\n",
    "The implementation includes the following components:\n",
    "- **Patcher**: Divides the input image into smaller patches.\n",
    "- **LinearProjectionFlatten**: Projects the patches into a higher-dimensional space.\n",
    "- **PositionalEncoder**: Adds positional information to the patches.\n",
    "- **SelfAttention**: Implements the self-attention mechanism.\n",
    "- **AttentionBlock**: Combines self-attention with linear projections.\n",
    "- **TransformerEncoder**: Stacks multiple attention blocks with normalization and feed-forward layers.\n",
    "- **MLPHead**: A simple feed-forward network for final classification.\n",
    "- **VisionTransformer**: Combines all components to form the complete Vision Transformer model.\n",
    "\n",
    "The model is trained on the CIFAR-10 dataset for 10 epochs using the Adam optimizer and cross-entropy loss. The training loss is plotted to visualize the training process, and the model's prediction for a sample image is compared with the true label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torchvision.transforms import transforms\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data (Cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple transform for data since real focus is on ViT\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# One-hot encoding the target labels\n",
    "def one_hot_encode(label):\n",
    "    return F.one_hot(torch.tensor(label), num_classes=10).float()\n",
    "\n",
    "train_dataset = CIFAR10(\n",
    "    root='../../../datasets',\n",
    "    train=True,\n",
    "    download=False, # Download the dataset if it does not exist\n",
    "    transform=transform,\n",
    "    target_transform=one_hot_encode\n",
    ")\n",
    "\n",
    "test_dataset = CIFAR10(\n",
    "    root='../../../datasets',\n",
    "    train=False,\n",
    "    download=False, # Download the dataset if it does not exist\n",
    "    transform=transform,\n",
    "    target_transform=one_hot_encode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will implement the Vision Transformer (ViT) architecture as described in its original paper: [\"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"](https://arxiv.org/abs/2010.11929).\n",
    "\n",
    "![Vision Transformer](ViT_Image_paper.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patcher\n",
    "\n",
    "The `Patcher` class is responsible for dividing the input image into smaller patches. Each patch is then flattened into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patcher(nn.Module):\n",
    "    def __init__(self, patch_size=16, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # Batch size, Num patches, Flatten features\n",
    "        patches = rearrange(x, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size)\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearProjectionFlatten\n",
    "\n",
    "The `LinearProjectionFlatten` class takes the patches created by the `Patcher` and projects them into a higher-dimensional space using a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProjectionFlatten(nn.Module):\n",
    "    def __init__(self, patcher: Patcher, num_channels=3):\n",
    "        super().__init__()\n",
    "        self.patcher: Patcher = patcher\n",
    "        self.patch_embedding = nn.Linear(num_channels * patcher.patch_size ** 2, patcher.embed_dim)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        patches = self.patcher(x)\n",
    "        embedded_patches = self.patch_embedding(patches)\n",
    "        return embedded_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PositionalEncoder\n",
    "\n",
    "The `PositionalEncoder` class adds positional information to the patches, which helps the model understand the order of the patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512, seq_len=5000):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        position = torch.arange(0, seq_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(np.log(10000.0) / embed_dim))\n",
    "\n",
    "        pe = torch.zeros(seq_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Using buffer to tell pytorch this is not learnable\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        x = x + self.pe[:seq_len, :].unsqueeze(dim=0)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelfAttention\n",
    "\n",
    "The `SelfAttention` class implements the self-attention mechanism, which allows the model to focus on different parts of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def calculate_QKT(self, queries: Tensor, keys: Tensor):\n",
    "        return torch.bmm(queries, keys.transpose(1, 2)) / np.sqrt(queries.shape[-1])\n",
    " \n",
    "    def forward(self, queries: Tensor, keys: Tensor, values: Tensor, \n",
    "                batch_size: int, seq_len: int, embed_dim: int, n_head: int):\n",
    "        \n",
    "        keys = keys.transpose(1, 2).contiguous().view(batch_size * n_head, seq_len, embed_dim)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(batch_size * n_head, seq_len, embed_dim)\n",
    "        values = values.transpose(1, 2).contiguous().view(batch_size * n_head, seq_len, embed_dim)\n",
    "\n",
    "        QKT = self.calculate_QKT(queries, keys)\n",
    "        scores = F.softmax(QKT, dim=2)\n",
    "        attention: Tensor = torch.bmm(scores, values).view(batch_size, n_head, seq_len, embed_dim)\n",
    "        attention = attention.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim * n_head)\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AttentionBlock\n",
    "\n",
    "The `AttentionBlock` class combines the self-attention mechanism with linear projections to form a complete attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=512, n_head=8):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_head = n_head\n",
    "\n",
    "        # W for queries\n",
    "        self.Wq = nn.Linear(embed_dim, embed_dim * n_head, bias=False)\n",
    "        # W for keys\n",
    "        self.Wk = nn.Linear(embed_dim, embed_dim * n_head, bias=False)\n",
    "        # W for values\n",
    "        self.Wv = nn.Linear(embed_dim, embed_dim * n_head, bias=False)\n",
    "        # W for final results\n",
    "        self.Wr = nn.Linear(embed_dim * n_head, embed_dim)\n",
    "\n",
    "        self.attention = SelfAttention()\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        n_head = self.n_head\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "\n",
    "        queries = self.Wq(x).view(batch_size, seq_len, n_head, embed_dim)\n",
    "        keys = self.Wk(x).view(batch_size, seq_len, n_head, embed_dim)\n",
    "        values = self.Wv(x).view(batch_size, seq_len, n_head, embed_dim)\n",
    "\n",
    "        result = self.attention(queries, keys, values, batch_size,\n",
    "                                seq_len, embed_dim, n_head)\n",
    "        \n",
    "        return self.Wr(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerEncoder\n",
    "\n",
    "The `TransformerEncoder` class stacks multiple attention blocks and adds normalization and feed-forward layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512, n_head=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = AttentionBlock(embed_dim, n_head)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        hidden_size = embed_dim * 2\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        sc = self.attention(x) # First skip connection\n",
    "        x = self.norm1(x + sc)\n",
    "        sc = self.mlp(x)\n",
    "        out = self.norm2(x + sc) # Second skip connection\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPHead\n",
    "\n",
    "The `MLPHead` class is a simple feed-forward network that produces the final classification logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, n_classes, embed_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embed_dim, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x_pool = x.mean(dim=1)\n",
    "        \n",
    "        logits = self.layers(x_pool)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisionTransformer\n",
    "\n",
    "The `VisionTransformer` class combines all the components to form the complete Vision Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, n_classes, patch_size=16, seq_len=5000, n_head=8, embed_dim=512, num_channels=3, n_transformers=6):\n",
    "        super().__init__()\n",
    "\n",
    "        patcher = Patcher(patch_size, embed_dim)\n",
    "        self.linear_flatter = LinearProjectionFlatten(patcher, num_channels)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoder(embed_dim, seq_len)\n",
    "\n",
    "        transformer_encoders = [TransformerEncoder(embed_dim, n_head) for _ in range(n_transformers)]\n",
    "        self.transformers = nn.Sequential(*transformer_encoders)\n",
    "\n",
    "        self.mlp_head = MLPHead(n_classes, embed_dim)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        flatten = self.linear_flatter(x)\n",
    "        \n",
    "        positional_encoded = self.positional_encoding(flatten)\n",
    "\n",
    "        transform = self.transformers(positional_encoded)\n",
    "        return self.mlp_head(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The model is trained on the CIFAR-10 dataset for 10 epochs using the Adam optimizer and cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:39<05:54, 39.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.9835, Test Loss: 1.9835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:14<04:54, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.6130, Test Loss: 1.6130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:50<04:14, 36.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 1.4168, Test Loss: 1.4168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [02:24<03:33, 35.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 1.3080, Test Loss: 1.3080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [03:05<03:06, 37.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 1.2195, Test Loss: 1.2195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [03:40<02:27, 36.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 1.1562, Test Loss: 1.1562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [04:20<01:53, 37.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 1.0930, Test Loss: 1.0930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [05:03<01:18, 39.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 1.0371, Test Loss: 1.0371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [05:38<00:37, 37.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.9898, Test Loss: 0.9898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [05:44<00:38, 38.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 28\u001b[0m     epoch_loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Using test dataset as validation dataset\u001b[39;00m\n\u001b[0;32m     31\u001b[0m epoch_loss_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = VisionTransformer(n_classes=10, patch_size=4, seq_len=5000, n_head=8,\n",
    "                        embed_dim=50, num_channels=3, n_transformers=6).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):  \n",
    "    num_batches = len(train_dataloader)\n",
    "\n",
    "    epoch_loss_train = 0\n",
    "    model.train()\n",
    "\n",
    "    for img, label in train_dataloader:\n",
    "        img: Tensor = img.to(device)\n",
    "        label: Tensor = label.to(device)\n",
    "\n",
    "        logits: Tensor = model(img)\n",
    "        loss: Tensor = loss_fn(logits, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss_train += loss.item()\n",
    "\n",
    "    # Using test dataset as validation dataset\n",
    "    epoch_loss_test = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for img, label in test_dataloader:\n",
    "            img: Tensor = img.to(device)\n",
    "            label: Tensor = label.to(device)\n",
    "\n",
    "            logits: Tensor = model(img)\n",
    "            loss: Tensor = loss_fn(logits, label)\n",
    "\n",
    "            epoch_loss_test = loss.item()\n",
    "\n",
    "    epoch_loss_train /= num_batches\n",
    "    epoch_loss_test /= num_batches\n",
    "    train_losses.append(epoch_loss_train)\n",
    "    test_losses.append(epoch_loss_test)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {epoch_loss_train:.4f}, Test Loss: {epoch_loss_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, len(range(epochs)) + 1), train_losses, label='train', color='blue')\n",
    "plt.plot(range(1, len(range(epochs)) + 1), test_losses, label='test', color='orange')\n",
    "plt.legend()\n",
    "plt.title('Loss for epoch')\n",
    "plt.xticks(range(1, epochs + 1))\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
