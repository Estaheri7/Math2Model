{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7dabf9",
   "metadata": {},
   "source": [
    "# Swin Transformer Implementation Using PyTorch\n",
    "\n",
    "This notebook showcases the implementation of a Swin Transformer from scratch using PyTorch. The Swin Transformer (Shifted Window Transformer) is a hierarchical vision transformer architecture that introduces window-based self-attention and shifted windowing to achieve computational efficiency and improved performance on vision tasks.\n",
    "\n",
    "We use a downsampled version of the MNIST dataset (resized to 32x32) or similar small grayscale datasets for demonstration. The model is kept lightweight for ease of understanding and training from scratch.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset used is the MNIST dataset, resized to **32x32**. The dataset consists of 70,000 grayscale images of handwritten digits (0-9). Each image has one channel and is labeled with a corresponding digit.\n",
    "\n",
    "- **Input Size**: 32×32×1\n",
    "- **Number of Classes**: 10\n",
    "\n",
    "## Swin Transformer Components\n",
    "\n",
    "The Swin Transformer is implemented with the following components:\n",
    "\n",
    "- **PatchPartition**: Splits the input image into non-overlapping patches and reshapes the tensor.\n",
    "- **LinearEmbedding**: Projects flattened patches to the desired embedding dimension.\n",
    "- **Window-based Multi-Head Self Attention (W-MSA)**: Performs self-attention within local non-overlapping windows.\n",
    "- **Shifted Window Multi-Head Self Attention (SW-MSA)**: Introduces overlapping context by shifting the windows.\n",
    "- **SwinTransformerBlock**: Core building block that alternates between W-MSA and SW-MSA.\n",
    "- **PatchMerging**: Downsamples the feature map by merging adjacent patches and increasing the embedding dimension.\n",
    "- **SwinTransformer**: Full model composed of stacked Swin Transformer blocks and hierarchical downsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdef229",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "148ec9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f1127f",
   "metadata": {},
   "source": [
    "## Load MNIST dataset and move it to dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "baf13f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = datasets.MNIST(\n",
    "    root='../../../datasets',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    ")\n",
    "\n",
    "dataset_test = datasets.MNIST(\n",
    "    root='../../../datasets',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    ")\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe706f7",
   "metadata": {},
   "source": [
    "# Swin Transformer\n",
    "\n",
    "The **Swin Transformer** is a hierarchical vision transformer that computes self-attention within local windows and shifts them between blocks. This design combines the benefits of both convolutional neural networks (CNNs) and Transformers — achieving linear computational complexity and strong inductive bias for vision tasks.\n",
    "\n",
    "![Swin Transformer](swinT_architecture.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Patch Partition\n",
    "\n",
    "The image is first partitioned into **non-overlapping patches** of size $P \\times P$.\n",
    "\n",
    "For an image $x \\in \\mathbb{R}^{B \\times C \\times H \\times W}$, the patch partition operation reshapes it to:\n",
    "\n",
    "$$\n",
    "x' \\in \\mathbb{R}^{B \\times \\frac{H}{P} \\times \\frac{W}{P} \\times (P^2 \\cdot C)}\n",
    "$$\n",
    "\n",
    "This operation is similar to convolution with:\n",
    "- Kernel size = patch size\n",
    "- Stride = patch size\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Linear Embedding\n",
    "\n",
    "Each patch (flattened vector of size $P^2 \\cdot C$) is projected to a fixed dimension $D$ using a learnable linear layer:\n",
    "\n",
    "$$\n",
    "z = xW_e + b_e, \\quad \\text{where } W_e \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D}\n",
    "$$\n",
    "\n",
    "The result is a feature map of shape:\n",
    "\n",
    "$$\n",
    "z \\in \\mathbb{R}^{B \\times \\frac{H}{P} \\times \\frac{W}{P} \\times D}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Swin Transformer Block\n",
    "\n",
    "### 3.1 Window-based Self-Attention\n",
    "\n",
    "Instead of global self-attention, the image is divided into **non-overlapping windows** of size $M \\times M$, and attention is computed within each window independently.\n",
    "\n",
    "Given input $x \\in \\mathbb{R}^{B \\times H \\times W \\times C}$, the window partition splits it into:\n",
    "\n",
    "$$\n",
    "\\text{windows} \\in \\mathbb{R}^{B \\cdot \\frac{H}{M} \\cdot \\frac{W}{M} \\times M \\times M \\times C}\n",
    "$$\n",
    "\n",
    "Self-attention within each window is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V$\n",
    "- $W^Q, W^K, W^V \\in \\mathbb{R}^{C \\times d}$\n",
    "\n",
    "### 3.2 Shifted Windows\n",
    "\n",
    "To enable cross-window interactions, **the windows are shifted** by $\\frac{M}{2}$ pixels in the next block.\n",
    "\n",
    "This operation:\n",
    "- Introduces connections across windows.\n",
    "- Preserves locality like CNNs.\n",
    "- Avoids quadratic cost of full attention.\n",
    "\n",
    "### 3.3 Attention Masking\n",
    "\n",
    "When using shifted windows, some tokens across different windows fall into the same attention region. To avoid unintended interactions, an **attention mask** is applied.\n",
    "\n",
    "Let the attention score matrix be $A \\in \\mathbb{R}^{M^2 \\times M^2}$, the mask $M \\in \\{0, -\\infty\\}^{M^2 \\times M^2}$ is added:\n",
    "\n",
    "$$\n",
    "\\text{MaskedAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}} + M\\right)V\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Patch Merging\n",
    "\n",
    "To downsample feature maps (like pooling), Swin Transformer merges every $2 \\times 2$ patch region:\n",
    "\n",
    "1. Reshape each $2 \\times 2$ neighborhood to a vector of size $4C$.\n",
    "2. Apply a linear layer $W \\in \\mathbb{R}^{4C \\times 2C}$.\n",
    "\n",
    "This reduces spatial dimensions by 2 and doubles feature dimension:\n",
    "\n",
    "$$\n",
    "\\text{Input: } B \\times H \\times W \\times C \\\\\n",
    "\\text{Output: } B \\times \\frac{H}{2} \\times \\frac{W}{2} \\times 2C\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Final Classification Head\n",
    "\n",
    "After all stages (each with multiple blocks and optional patch merging), the resulting feature map is normalized and pooled:\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^{B \\times H' \\times W' \\times C} \\\\\n",
    "x = \\text{LayerNorm}(x) \\\\\n",
    "x = \\text{AvgPool}(x) \\rightarrow \\mathbb{R}^{B \\times C} \\\\\n",
    "\\hat{y} = \\text{Linear}(x) \\rightarrow \\mathbb{R}^{B \\times \\text{num\\_classes}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summary Table\n",
    "\n",
    "| Component             | Role                                                        | Key Idea |\n",
    "|-----------------------|-------------------------------------------------------------|----------|\n",
    "| Patch Partition       | Divide input into flat patches                              | CNN-like stride operation |\n",
    "| Linear Embedding      | Map patches to embedding space                              | FC projection |\n",
    "| Window Self-Attention | Attention within fixed-size windows                         | Reduces complexity |\n",
    "| Shifted Windows       | Alternate blocks with shifted windows                       | Cross-window interaction |\n",
    "| Patch Merging         | Spatial downsampling and channel upsampling                 | Hierarchical |\n",
    "| LayerNorm + Head      | Normalize + aggregate features + predict class              | Standard |\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Complexity\n",
    "\n",
    "Let:\n",
    "- Image size: $H \\times W$\n",
    "- Patch size: $P$\n",
    "- Window size: $M$\n",
    "- Number of channels: $C$\n",
    "\n",
    "Then:\n",
    "- Global attention: $O((HW)^2)$\n",
    "- Swin window attention: $O\\left(\\frac{HW}{M^2} \\cdot M^4\\right) = O(HW \\cdot M^2)$\n",
    "\n",
    "Hence, Swin attention scales **linearly** with image size (like CNNs), compared to quadratic scaling in ViT.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- [Swin Transformer Paper](https://arxiv.org/abs/2103.14030)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3afe2b8",
   "metadata": {},
   "source": [
    "### PatchPartition\n",
    "\n",
    "The `PatchPartition` class splits the input image into non-overlapping patches of size `(patch_size × patch_size)`, then flattens and reshapes the result into shape `(B, H//p, W//p, patch_dim)`, where `patch_dim = patch_size² × channels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b50912e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchPartition(nn.Module):\n",
    "    def __init__(self, in_channels, height, width, patch_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_patch = (height // patch_size) * (width // patch_size)\n",
    "\n",
    "        self.partition = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.partition(x)\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db6af3b",
   "metadata": {},
   "source": [
    "### LinearEmbedding\n",
    "\n",
    "The `LinearEmbedding` class projects the flattened patch features into a higher-dimensional embedding space (e.g., 32 or 96) using a linear layer. This is analogous to word embedding in NLP transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7836fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEmbedding(nn.Module):\n",
    "    def __init__(self, from_dim, to_dim):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(from_dim, to_dim)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return self.projection(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2677e74c",
   "metadata": {},
   "source": [
    "## Window Partition & Reverse — Swin Transformer Helper Functions\n",
    "\n",
    "In the Swin Transformer, self-attention is computed inside **local windows** rather than across the full image, significantly reducing computational complexity. Two crucial operations that enable this behavior are:\n",
    "\n",
    "- `window_partition`: splits a feature map into non-overlapping windows.\n",
    "- `window_reverse`: reconstructs the original spatial layout from the set of windows.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Window Partition\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function takes a feature map with shape:\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^{B \\times H \\times W \\times C}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- B is the batch size,\n",
    "- H, W are the height and width,\n",
    "- C is the number of channels,\n",
    "\n",
    "and splits it into **non-overlapping windows** of shape $ M \\times M$, where M is the window size.\n",
    "\n",
    "### Mathematics Behind\n",
    "\n",
    "1. **Reshaping**:  \n",
    "   The feature map is reshaped into:\n",
    "\n",
    "   $$\n",
    "   x' \\in \\mathbb{R}^{B \\times \\frac{H}{M} \\times M \\times \\frac{W}{M} \\times M \\times C}\n",
    "   $$\n",
    "\n",
    "2. **Permutation**:  \n",
    "   Dimensions are permuted to group window blocks together:\n",
    "\n",
    "   $$\n",
    "   x'' \\in \\mathbb{R}^{B \\times \\frac{H}{M} \\times \\frac{W}{M} \\times M \\times M \\times C}\n",
    "   $$\n",
    "\n",
    "3. **Flattening**:  \n",
    "   The reshaped tensor is flattened so that all windows are treated as independent inputs:\n",
    "\n",
    "   $$\n",
    "   \\text{windows} \\in \\mathbb{R}^{(B \\cdot \\frac{H}{M} \\cdot \\frac{W}{M}) \\times M \\times M \\times C}\n",
    "   $$\n",
    "\n",
    "These windows are then used to apply **local self-attention** efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Window Reverse\n",
    "\n",
    "### Purpose\n",
    "\n",
    "After computing attention inside windows, this function reverses the operation and reconstructs the original feature map of shape:\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^{B \\times H \\times W \\times C}\n",
    "$$\n",
    "\n",
    "from the windows of shape:\n",
    "\n",
    "$$\n",
    "\\text{windows} \\in \\mathbb{R}^{(B \\cdot \\frac{H}{M} \\cdot \\frac{W}{M}) \\times M \\times M \\times C}\n",
    "$$\n",
    "\n",
    "### Mathematics Behind\n",
    "\n",
    "1. **Determine Batch Size**:  \n",
    "   The batch size B is inferred from the number of windows:\n",
    "\n",
    "   $$\n",
    "   B = \\frac{\\text{num\\_windows}}{\\frac{H}{M} \\cdot \\frac{W}{M}}\n",
    "   $$\n",
    "\n",
    "2. **Reshape**:  \n",
    "   The windows are reshaped into the grouped format:\n",
    "\n",
    "   $$\n",
    "   x' \\in \\mathbb{R}^{B \\times \\frac{H}{M} \\times \\frac{W}{M} \\times M \\times M \\times C}\n",
    "   $$\n",
    "\n",
    "3. **Permute & Merge**:  \n",
    "   The spatial layout is restored by permuting the axes and merging the dimensions:\n",
    "\n",
    "   $$\n",
    "   x \\in \\mathbb{R}^{B \\times H \\times W \\times C}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## Why These Functions Matter\n",
    "\n",
    "- These operations enable the Swin Transformer to perform **efficient local attention** while still maintaining the ability to aggregate global information through **shifted windows** in successive layers.\n",
    "- The key benefit is the **linear scaling** of attention cost with image size:\n",
    "  $$\n",
    "  \\text{Complexity} = O\\left(\\frac{HW}{M^2} \\cdot M^4\\right) = O(HW \\cdot M^2)\n",
    "  $$\n",
    "\n",
    "Compared to standard global attention:\n",
    "  $$\n",
    "  O((HW)^2)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Function          | Input Shape                           | Output Shape                          | Purpose                                |\n",
    "|------------------|----------------------------------------|----------------------------------------|----------------------------------------|\n",
    "| `window_partition` |$ B \\times H \\times W \\times C $     | $ (B \\cdot \\frac{H}{M} \\cdot \\frac{W}{M}) \\times M \\times M \\times C $ | Split into windows                     |\n",
    "| `window_reverse`   | $(B \\cdot \\frac{H}{M} \\cdot \\frac{W}{M}) \\times M \\times M \\times C $ |  $B \\times H \\times W \\times C $     | Reconstruct the original feature map   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13e2902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x: Tensor, window_size: int):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous()  # (B, H//ws, W//ws, ws, ws, C)\n",
    "    windows = x.view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows: Tensor, window_size: int, H: int, W: int):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "    x = x.view(B, H, W, -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac543f8c",
   "metadata": {},
   "source": [
    "# WindowAttention\n",
    "\n",
    "The **WindowAttention** module implements **local self-attention** within fixed‐size windows of the feature map. It is a core building block of the Swin Transformer, enabling efficient computation and spatial inductive bias.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Purpose & Motivation\n",
    "\n",
    "- **Locality**  \n",
    "  By restricting attention to a window of size $M \\times M$, we reduce complexity from  \n",
    "  $$O\\bigl((HW)^2\\bigr)\\quad\\text{to}\\quad O\\!\\Bigl(\\tfrac{HW}{M^2}\\,M^4\\Bigr)=O(HW\\,M^2).$$\n",
    "\n",
    "- **Shifted Windows**  \n",
    "  Alternating between regular and shifted windows enables cross‐window interactions without global attention.\n",
    "\n",
    "- **Spatial Bias**  \n",
    "  Injecting learnable **relative position biases** helps the model encode spatial relationships within each window.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Mathematical Details\n",
    "\n",
    "### 2.1 Input & QKV Projection\n",
    "\n",
    "Given input tokens  \n",
    "$$X \\in \\mathbb{R}^{B N \\,\\times\\, N \\,\\times\\, D},$$  \n",
    "where $B$ is batch size, $N=M^2$ is tokens per window, and $D$ is embedding dim, we compute:  \n",
    "$$[Q,K,V] = X\\,W_{qkv},\\quad W_{qkv}\\in\\mathbb{R}^{D\\times 3D},$$  \n",
    "then reshape into $h$ heads of size $d=D/h$:  \n",
    "$$Q,K,V\\in\\mathbb{R}^{B N\\times h\\times N\\times d}.$$\n",
    "\n",
    "### 2.2 Scaled Dot‐Product Attention\n",
    "\n",
    "For each head:  \n",
    "$$A = \\frac{Q\\,K^\\top}{\\sqrt{d}}\\ \\in\\mathbb{R}^{B N\\times N\\times N},$$  \n",
    "computing affinities between all token pairs within a window.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Relative Position Bias\n",
    "\n",
    "### 3.1 Creation of the Bias Table\n",
    "\n",
    "We introduce a learnable **bias table**  \n",
    "$$B_r \\in \\mathbb{R}^{(2M-1)^2 \\times h},$$  \n",
    "where each of the $(2M-1)^2$ entries corresponds to a relative offset $(\\Delta x,\\Delta y)$ between two tokens in an $M\\times M$ window.  \n",
    "- Offsets range $\\Delta x,\\Delta y\\in\\{-M+1,\\dots,M-1\\}$.  \n",
    "- We map each offset to a 1D index via  \n",
    "  $$\\text{idx}(\\Delta x,\\Delta y) = (\\Delta x + M - 1)\\,(2M-1) + (\\Delta y + M - 1).$$  \n",
    "- The table is initialized with a **truncated normal** distribution:  \n",
    "  $$B_r[i,k]\\sim\\mathcal{N}(0,\\sigma^2)\\text{ clipped to }[-2\\sigma,2\\sigma],\\quad \\sigma=0.02.$$\n",
    "\n",
    "### 3.2 Adding Bias to Attention\n",
    "\n",
    "We precompute an index tensor  \n",
    "$$\\mathrm{idx}\\in\\{0,\\dots,(2M-1)^2-1\\}^{N\\times N}$$  \n",
    "that maps each token‐pair $(i,j)$ to its bias index. Then the adjusted logits become  \n",
    "$$\\widetilde{A}_{b,i,j}^{(k)} = A_{b,i,j}^{(k)} \\;+\\; B_r[\\mathrm{idx}_{i,j},\\,k].$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Softmax & Output\n",
    "\n",
    "1. **Softmax** over the last dimension:  \n",
    "   $$\\alpha = \\mathrm{softmax}\\bigl(\\widetilde{A}\\bigr)\\in\\mathbb{R}^{B N\\times N\\times N}.$$\n",
    "2. **Aggregate** values:  \n",
    "   $$O = \\alpha\\,V \\;\\in\\mathbb{R}^{B N\\times h\\times N\\times d}.$$\n",
    "3. **Merge heads** and apply final linear projection to restore shape  \n",
    "   $$O'\\in\\mathbb{R}^{B N\\times N\\times D}.$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Why It’s Necessary\n",
    "\n",
    "- **Efficiency**: Local windows reduce compute on high‐res inputs.  \n",
    "- **Cross‐window Links**: Shifted windows connect neighboring regions without full attention.  \n",
    "- **Inductive Bias**: Relative biases encode spatial priors (nearby patches more related) while remaining learnable.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summary\n",
    "\n",
    "| Step                      | Operation & Formula                                                           |\n",
    "|---------------------------|-------------------------------------------------------------------------------|\n",
    "| **QKV Projection**        | $[Q,K,V]=XW_{qkv},\\;Q,K,V\\in\\mathbb{R}^{BN\\times h\\times N\\times d}$          |\n",
    "| **Scaled Dot‐Product**    | $A=\\tfrac{QK^\\top}{\\sqrt{d}}$                                                  |\n",
    "| **Relative Bias**         | $\\widetilde{A}_{i,j}^{(k)}=A_{i,j}^{(k)}+B_r[\\mathrm{idx}_{i,j},k]$            |\n",
    "| **Softmax & Aggregate**   | $\\alpha=\\mathrm{softmax}(\\widetilde{A}),\\;O=\\alpha V$                          |\n",
    "| **Merge & Project**       | Concatenate heads, linear map $hd\\to D$                                        |\n",
    "\n",
    "The **WindowAttention** module balances computational tractability with the ability to capture both **local** and, via shifts, **cross‐window** interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec3e9636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, window_size, num_heads, shift_size, input_resolution):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.num_heads = num_heads\n",
    "        self.input_resolution = input_resolution\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        # Position bias table\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2*window_size-1)*(2*window_size-1), num_heads)\n",
    "        )\n",
    "        nn.init.trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "\n",
    "        # Relative position index for each token inside a window\n",
    "        self.register_buffer(\"relative_position_index\", self.get_relative_position_index())\n",
    "\n",
    "    def get_relative_position_index(self):\n",
    "        \"\"\"Build relative position index tensor.\"\"\"\n",
    "        coords = torch.stack(torch.meshgrid(\n",
    "            torch.arange(self.window_size),\n",
    "            torch.arange(self.window_size), indexing=\"ij\"\n",
    "        ))  # (2, window_size, window_size)\n",
    "        coords_flatten = coords.flatten(1)  # (2, M^2)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # (2, M^2, M^2)\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # (M^2, M^2, 2)\n",
    "        relative_coords[:, :, 0] += self.window_size - 1  # shift to 0-based index\n",
    "        relative_coords[:, :, 1] += self.window_size - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # (M^2, M^2)\n",
    "        return relative_position_index\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B*N, M^2, D)\n",
    "        Returns:\n",
    "            out: (B*N, M^2, D)\n",
    "        \"\"\"\n",
    "        B_, N, D = x.shape  # B*N = number of windows, N = M^2\n",
    "\n",
    "        # Step 1: Project input to Q, K, V\n",
    "        qkv = self.qkv(x)  # (B*N, N, 3*D)\n",
    "        qkv = qkv.reshape(B_, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)  # (3, B*N, h, N, d)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # each: (B*N, h, N, d)\n",
    "\n",
    "        # Step 2: Scaled dot-product attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B*N, h, N, N)\n",
    "\n",
    "        # Step 3: Add relative positional bias\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)]\n",
    "        relative_position_bias = relative_position_bias.view(N, N, -1).permute(2, 0, 1)  # (h, N, N)\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)  # (1, h, N, N)\n",
    "\n",
    "        # Step 4: Softmax and apply attention\n",
    "        attn = attn.softmax(dim=-1)  # (B*N, h, N, N)\n",
    "        out = (attn @ v)  # (B*N, h, N, d)\n",
    "\n",
    "        # Step 5: Merge heads and project\n",
    "        out = out.transpose(1, 2).reshape(B_, N, D)  # (B*N, N, D)\n",
    "        out = self.proj(out)  # final linear projection\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83272f",
   "metadata": {},
   "source": [
    "### MLP (Multi-Layer Perceptron)\n",
    "\n",
    "The **MLP** (Multi-Layer Perceptron) class implements a simple feed-forward neural network with two linear layers. The network also includes **GELU** activations and **dropout** regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3b81606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim=None, dropout=0.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or 4 * dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c709bd2",
   "metadata": {},
   "source": [
    "### SwinTransformerBlock\n",
    "\n",
    "The `SwinTransformerBlock` is the main processing unit. It contains two stages:\n",
    "- **W-MSA** (Window-based Multi-head Self-Attention)\n",
    "- **SW-MSA** (Shifted Window Multi-head Self-Attention)\n",
    "\n",
    "Shifted windows allow information to flow between windows, maintaining global coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e29b7f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Norm and attention\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=window_size, num_heads=num_heads,\n",
    "            shift_size=shift_size, input_resolution=input_resolution\n",
    "        )\n",
    "\n",
    "        # MLP part\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, hidden_dim=int(dim * mlp_ratio))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, H, W, C)\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, _, _, C = x.shape\n",
    "        shortcut = x\n",
    "\n",
    "        # Step 1: cyclic shift if needed\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # Step 2: partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # (num_windows*B, ws, ws, C)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
    "\n",
    "        # Step 3: attention\n",
    "        attn_windows = self.attn(self.norm1(x_windows))  # (num_windows*B, ws*ws, C)\n",
    "\n",
    "        # Step 4: merge windows back\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        x = window_reverse(attn_windows, self.window_size, H, W)  # (B, H, W, C)\n",
    "\n",
    "        # Step 5: reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "\n",
    "        # Step 6: Residual + MLP\n",
    "        x = shortcut + x\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86046b",
   "metadata": {},
   "source": [
    "### PatchMerging\n",
    "\n",
    "The `PatchMerging` class is used for **downsampling**. It concatenates neighboring patches (e.g., a 2×2 grid), applies a linear layer, and reduces the spatial resolution by 2 while increasing the feature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d92b5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, input_resolution, dim):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = nn.LayerNorm(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, H, W, C)\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, _, _, C = x.shape\n",
    "\n",
    "        x = x.view(B, H // 2, 2, W // 2, 2, C)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).reshape(B, H // 2, W // 2, 4 * C)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a70c543",
   "metadata": {},
   "source": [
    "### SwinTransformer\n",
    "\n",
    "The `SwinTransformer` class combines:\n",
    "- Patch Partition\n",
    "- Embedding\n",
    "- Two Stages:\n",
    "  - Stage 1: W-MSA + SW-MSA + Patch Merging\n",
    "  - Stage 2: W-MSA + SW-MSA\n",
    "- Global pooling + classification head\n",
    "\n",
    "Each stage doubles the feature dimension and halves the resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a00ba65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=1, embed_dim=32, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchPartition(in_channels=in_chans, height=img_size, width=img_size,\n",
    "                                          patch_size=patch_size, embed_dim=4 * 4 * 1)\n",
    "        self.linear_embed = LinearEmbedding(from_dim=4 * 4 * 1, to_dim=embed_dim)\n",
    "\n",
    "        H = W = img_size // patch_size  # 32 / 4 = 8\n",
    "        self.stage1_block1 = SwinTransformerBlock(\n",
    "            dim=embed_dim, input_resolution=(H, W), num_heads=2, window_size=2, shift_size=0\n",
    "        )\n",
    "        self.stage1_block2 = SwinTransformerBlock(\n",
    "            dim=embed_dim, input_resolution=(H, W), num_heads=2, window_size=2, shift_size=1\n",
    "        )\n",
    "\n",
    "        # Apply Patch Merging: 8×8 → 4×4\n",
    "        self.patch_merging = PatchMerging(input_resolution=(H, W), dim=embed_dim)\n",
    "        H, W = H // 2, W // 2\n",
    "        embed_dim *= 2\n",
    "\n",
    "        self.stage2_block1 = SwinTransformerBlock(\n",
    "            dim=embed_dim, input_resolution=(H, W), num_heads=2, window_size=2, shift_size=0\n",
    "        )\n",
    "        self.stage2_block2 = SwinTransformerBlock(\n",
    "            dim=embed_dim, input_resolution=(H, W), num_heads=2, window_size=2, shift_size=1\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)  # (B, H//p, W//p, C)\n",
    "        x = self.linear_embed(x)\n",
    "\n",
    "        x = self.stage1_block1(x)\n",
    "        x = self.stage1_block2(x)\n",
    "\n",
    "        x = self.patch_merging(x)  # downsampling\n",
    "        x = self.stage2_block1(x)\n",
    "        x = self.stage2_block2(x)\n",
    "\n",
    "        B, H, W, C = x.shape\n",
    "        x = self.norm(x.view(B, H * W, C))\n",
    "        x = self.avgpool(x.transpose(1, 2)).squeeze(-1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c474c2",
   "metadata": {},
   "source": [
    "## Train the model on resized MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f10a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SwinTransformer().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe27623c",
   "metadata": {},
   "source": [
    "### train epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e8f3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss: Tensor = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    acc = total_correct / len(loader.dataset)\n",
    "    return avg_loss, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8e24c",
   "metadata": {},
   "source": [
    "### Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "213b6c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    acc = total_correct / len(loader.dataset)\n",
    "    return avg_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0380a54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 938/938 [00:16<00:00, 55.79it/s]\n",
      "Evaluating: 100%|██████████| 157/157 [00:01<00:00, 92.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4814 | Train Acc: 84.34%\n",
      "Val   Loss: 0.1425 | Val   Acc: 95.63%\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 938/938 [00:16<00:00, 56.38it/s]\n",
      "Evaluating: 100%|██████████| 157/157 [00:01<00:00, 95.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1294 | Train Acc: 96.12%\n",
      "Val   Loss: 0.1263 | Val   Acc: 95.90%\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 938/938 [00:16<00:00, 58.31it/s]\n",
      "Evaluating: 100%|██████████| 157/157 [00:01<00:00, 94.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0993 | Train Acc: 96.91%\n",
      "Val   Loss: 0.0929 | Val   Acc: 97.02%\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 938/938 [00:16<00:00, 56.21it/s]\n",
      "Evaluating: 100%|██████████| 157/157 [00:01<00:00, 96.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0793 | Train Acc: 97.52%\n",
      "Val   Loss: 0.0710 | Val   Acc: 97.73%\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 938/938 [00:16<00:00, 56.17it/s]\n",
      "Evaluating: 100%|██████████| 157/157 [00:01<00:00, 93.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0695 | Train Acc: 97.85%\n",
      "Val   Loss: 0.0818 | Val   Acc: 97.40%\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 938/938 [00:16<00:00, 55.97it/s]\n",
      "Evaluating: 100%|██████████| 157/157 [00:01<00:00, 97.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0409 | Train Acc: 98.73%\n",
      "Val   Loss: 0.0482 | Val   Acc: 98.34%\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 938/938 [00:16<00:00, 56.39it/s]\n",
      "Evaluating: 100%|██████████| 157/157 [00:01<00:00, 93.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0355 | Train Acc: 98.88%\n",
      "Val   Loss: 0.0560 | Val   Acc: 98.25%\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 938/938 [00:16<00:00, 56.61it/s]\n",
      "Evaluating: 100%|██████████| 157/157 [00:01<00:00, 97.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0315 | Train Acc: 99.00%\n",
      "Val   Loss: 0.0475 | Val   Acc: 98.44%\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 938/938 [00:16<00:00, 56.71it/s]\n",
      "Evaluating: 100%|██████████| 157/157 [00:01<00:00, 92.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0292 | Train Acc: 99.08%\n",
      "Val   Loss: 0.0444 | Val   Acc: 98.53%\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 938/938 [00:16<00:00, 56.01it/s]\n",
      "Evaluating: 100%|██████████| 157/157 [00:01<00:00, 94.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0254 | Train Acc: 99.23%\n",
      "Val   Loss: 0.0497 | Val   Acc: 98.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    train_loss, train_acc = train(model, dataloader_train, optimizer, criterion, device)\n",
    "    test_loss, test_acc = evaluate(model, dataloader_test, criterion, device)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"Val   Loss: {test_loss:.4f} | Val   Acc: {test_acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GLOBAL",
   "language": "python",
   "name": "global"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
