{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "A general layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input_ = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input_: np.ndarray):\n",
    "        raise NotImplementedError('This method should be implemented')\n",
    "\n",
    "    def backward(self, upstream_grad: np.ndarray):\n",
    "        raise NotImplementedError('This method should be implemented')\n",
    "\n",
    "    def step(self, lr):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Fully connected` label\n",
    "\n",
    "To calculate forward phase:\n",
    "\n",
    "### $Y = XW + B$\n",
    "\n",
    "Where `Y` is `output`, `X` is `dataset` (input), `W` is `weight matrix` and `B` is `bias matrix`\n",
    "\n",
    "- To update weights we use back propagation algorithm and gradient descend:\n",
    "\n",
    "    $ \\frac{\\partial L}{\\partial W} = \\frac{\\partial Y}{\\partial W} . \\frac{\\partial L}{\\partial Y}$\n",
    "\n",
    "    $\\frac{\\partial Y}{\\partial W} = X^T$\n",
    "\n",
    "    => $\\frac{\\partial L}{\\partial W} = X^T . \\frac{\\partial L}{\\partial Y}$\n",
    "\n",
    "- To calculate upstream gradient for the previous layer (downstream gradient):\n",
    "\n",
    "    $ \\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} . \\frac{\\partial Y}{\\partial X}$\n",
    "\n",
    "    $\\frac{\\partial Y}{\\partial X} = W^T$\n",
    "\n",
    "    => $\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} . W^T$\n",
    "\n",
    "- For updating bias we can calculate:\n",
    "\n",
    "    $ \\frac{\\partial L}{\\partial B} = \\sum_{i=1}^{m}\\frac{\\partial L}{\\partial \\mathbf{y}_i}$\n",
    "\n",
    "    the gradient $\\frac{\\partial L}{\\partial \\mathbf{y}_i}$ represents the loss derivative with respect to the output for the i-th sample\n",
    "\n",
    "Where `L` is Loss function and $\\frac{\\partial L}{\\partial Y}$ is upstream gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use Adam gradient update algorithm for updating weights and bias\n",
    "\n",
    "Adam optimization is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing. The algorithm leverages the power of adaptive learning rates to find individual learning rates for each parameter.\n",
    "\n",
    "The Adam update rules are as follows:\n",
    "\n",
    "1. Compute the moving averages of the gradients and the squared gradients:\n",
    "    $$\n",
    "    m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "    $$\n",
    "    $$\n",
    "    v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "    $$\n",
    "\n",
    "2. Correct the bias in the first and second moment estimates:\n",
    "    $$\n",
    "    \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n",
    "    $$\n",
    "    $$\n",
    "    \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "    $$\n",
    "\n",
    "3. Update the parameters:\n",
    "    $$\n",
    "    \\omega_t = \\omega_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "    $$\n",
    "\n",
    "Where:\n",
    "- $\\beta_1$ and $\\beta_2$ are the decay rates for the moment estimates\n",
    "- $\\epsilon$ is a small constant to prevent division by zero.\n",
    "- $\\alpha$ is the learning rate.\n",
    "- $g_t$ is the gradient at time step \\( t \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, features_in, features_out, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.w = np.random.randn(features_in, features_out) * np.sqrt(2.0 / features_in)\n",
    "        self.b = np.random.randn(1, features_out) * 0.1\n",
    "\n",
    "        self.gradw = None\n",
    "        self.gradb = None\n",
    "\n",
    "        self.mw = np.zeros_like(self.w)\n",
    "        self.vw = np.zeros_like(self.w)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.vb = np.zeros_like(self.b)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "\n",
    "    def forward(self, input_: np.ndarray):\n",
    "        self.input_ = input_\n",
    "        self.output = (input_ @ self.w) + self.b \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, upstream_grad: np.ndarray):\n",
    "        self.gradw = self.input_.T @ upstream_grad\n",
    "        self.gradb = np.sum(upstream_grad, axis=0, keepdims=True)\n",
    "\n",
    "        downstream_grad = upstream_grad @ self.w.T\n",
    "        return downstream_grad\n",
    "\n",
    "    def step(self, lr=0.1):\n",
    "        # Adam optimaztion\n",
    "        self.t += 1\n",
    "\n",
    "        self.mw = self.beta1 * self.mw + (1 - self.beta1) * self.gradw\n",
    "        self.vw = self.beta2 * self.vw + (1 - self.beta2) * (self.gradw ** 2)\n",
    "        mw_hat = self.mw / (1 - self.beta1 ** self.t)\n",
    "        vw_hat = self.vw / (1 - self.beta2 ** self.t)\n",
    "\n",
    "        self.w -= lr * mw_hat / (np.sqrt(vw_hat) + self.epsilon)\n",
    "\n",
    "        self.mb = self.beta1 * self.mb + (1 - self.beta1) * self.gradb\n",
    "        self.vb = self.beta2 * self.vb + (1 - self.beta2) * (self.gradb ** 2)\n",
    "\n",
    "        mb_hat = self.mb / (1 - self.beta1 ** self.t)\n",
    "        vb_hat = self.vb / (1 - self.beta2 ** self.t)\n",
    "\n",
    "        self.b -= lr * mb_hat / (np.sqrt(vb_hat) + self.epsilon)\n",
    "    def __str__(self):\n",
    "        return f'I am linear!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid function is defined as:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 - e^{-z}}\n",
    "$$\n",
    "which is forward phase.\n",
    "\n",
    "and for the backward phase we need to get the first derivative of sigmoid function which is:\n",
    "\n",
    "$$\n",
    "\\frac{d\\sigma(z)}{dz} = \\sigma(z)\\sigma(1 - z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def forward(self, input_: np.ndarray):\n",
    "        clipped_input = np.clip(input_, -500, 500)\n",
    "        self.output = 1 / (1 + np.exp(-input_))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, upstream_grad: np.ndarray):\n",
    "        downstream_grad = upstream_grad * (self.output * (1 - self.output))\n",
    "        return downstream_grad\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'I am sigmoid!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RELU layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu function is defined as:\n",
    "\n",
    "$$\n",
    "ReLU(z) = max(0, z)\n",
    "$$\n",
    "which is forward phase.\n",
    "\n",
    "and for the backward phase we take the first derivative of Relu:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dz} \\text{ReLU}(z) = \\begin{cases} \n",
    "0 & \\text{if } x \\leq 0, \\\\\n",
    "1 & \\text{if } x > 0.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def forward(self, input_: np.ndarray):\n",
    "        self.input_ = input_\n",
    "        self.output = np.maximum(0, input_)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, upstream_grad: np.ndarray):\n",
    "        downstream_grad = upstream_grad * (self.input_ > 0 ).astype(upstream_grad.dtype)\n",
    "        return downstream_grad\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'I am ReLU'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax function for the forward phase is defined as:\n",
    "\n",
    "$$\\text{Softmax}(\\mathbf{z_i}) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$$\n",
    "\n",
    "\n",
    "and for the backward phase we have to calculate the jaccobian matrix:\n",
    "\n",
    "since\n",
    "$\\frac{\\partial S_i}{\\partial z_j} = -S_iS_j$\n",
    "and\n",
    "$\\frac{\\partial S_i}{\\partial z_i} = S_i(1-S_i)$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S}{\\partial z} = \\mathbf{J} =\n",
    "\\begin{bmatrix}\n",
    "S_1(1 - S_1) & -S_1 S_2 & -S_1 S_3 & \\cdots & -S_1 S_C \\\\\n",
    "-S_2 S_1 & S_2(1 - S_2) & -S_2 S_3 & \\cdots & -S_2 S_C \\\\\n",
    "-S_3 S_1 & -S_3 S_2 & S_3(1 - S_3) & \\cdots & -S_3 S_C \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "-S_C S_1 & -S_C S_2 & -S_C S_3 & \\cdots & S_C(1 - S_C)\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def forward(self, input_: np.ndarray):\n",
    "        # to prevent overflow\n",
    "        fixed_values = input_ - np.max(input_, axis=1, keepdims=True)\n",
    "        scores = np.exp(fixed_values)\n",
    "        sum_scores = np.sum(scores, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = scores / sum_scores\n",
    "        return self.output\n",
    "    def backward(self, upstream_grad: np.ndarray):\n",
    "        downstream_grad = np.empty_like(upstream_grad)\n",
    "\n",
    "        for idx in range(upstream_grad.shape[0]):\n",
    "            out = self.output[idx]\n",
    "            j = np.diag(out) - np.outer(out, out) # create jacobian matrix\n",
    "            \n",
    "            downstream_grad[idx] = j @ upstream_grad[idx]\n",
    "\n",
    "        return downstream_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error (MSE) function measures the average of the squares of the errorsâ€”that is, the average squared difference between the estimated values and the actual value.\n",
    "\n",
    "The `MSE` class implements this function in both the forward and backward phases.\n",
    "\n",
    "For the forward phase, the formula is:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{pred_i} - y_{true_i})^2\n",
    "$$\n",
    "\n",
    "For the backward phase, the gradient of the loss with respect to the predictions is:\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial y_{pred}} = \\frac{2}{n} (y_{pred} - y_{true})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Layer):\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        self.output = np.mean((y_pred - y_true) ** 2)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self):\n",
    "        upstream_grad = (2 / self.y_pred.shape[0]) * (self.y_pred - self.y_true)\n",
    "        return upstream_grad\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'I am MSE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cross Entropy loss function is commonly used in classification problems. It measures the performance of a classification model whose output is a probability value between 0 and 1. The loss increases as the predicted probability diverges from the actual label.\n",
    "\n",
    "For the forward phase, the formula is:\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})\n",
    "$$\n",
    "where:\n",
    "- $N$ is the number of samples\n",
    "- $C$ is the number of classes\n",
    "- $y_{i,c}$ is the true label (one-hot encoded) for sample \\( i \\) and class \\( c \\)\n",
    "- $\\hat{y}_{i,c}$ is the predicted probability for sample \\( i \\) and class \\( c \\)\n",
    "\n",
    "For the backward phase, the gradient of the loss with respect to the predictions is:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} = -\\frac{1}{N} \\frac{y}{\\hat{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(Layer):\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "\n",
    "        # To prevent undefined values\n",
    "        clip_y_pred = np.clip(y_pred, 1e-12, 1.0)\n",
    "\n",
    "        self.clip_y_pred = clip_y_pred\n",
    "\n",
    "        self.loss = -np.mean(np.sum(self.y_true * np.log(self.clip_y_pred), axis=1))\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self):\n",
    "        upstream_grad = -self.y_true / self.clip_y_pred / self.y_true.shape[0]\n",
    "        return upstream_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    A simple Multi-Layer Perceptron (MLP) class for training neural networks.\n",
    "\n",
    "    This class defines an MLP with customizable layers, a loss function, and an optimization procedure.\n",
    "    It supports forward and backward propagation, as well as parameter updates using a specified learning rate.\n",
    "\n",
    "    Attributes:\n",
    "        layers (list[Layer]): A list of Layer objects defining the structure of the MLP.\n",
    "        loss_method (Layer): A loss function layer used to compute the error and gradients during training.\n",
    "        lr (float): The learning rate used for updating weights in the network.\n",
    "        outputs (np.ndarray): A container for the output of the network during the forward pass.\n",
    "\n",
    "    Methods:\n",
    "        forward(input_: np.ndarray) -> np.ndarray:\n",
    "            Performs a forward pass through the network using the given input data.\n",
    "\n",
    "        backward() -> None:\n",
    "            Computes the gradients via backpropagation starting from the loss function.\n",
    "\n",
    "        update_weigths() -> None:\n",
    "            Updates the weights of all layers in the network using the calculated gradients and learning rate.\n",
    "\n",
    "        train(input_: np.ndarray, y: np.ndarray, epoches=100, batch_size=1) -> list:\n",
    "            Trains the MLP on the provided data (`input_` and `y`) for a specified number of epochs.\n",
    "            Returns the list of training losses over the epochs.\n",
    "\n",
    "        predict(input_: np.ndarray) -> np.ndarray:\n",
    "            Performs a forward pass for prediction with the given input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers: list[Layer], loss_method: Layer, lr=0.01):\n",
    "        \"\"\"\n",
    "        Initializes the MLP with the given layers, loss function, and learning rate.\n",
    "\n",
    "        Args:\n",
    "            layers (list[Layer]): A list of Layer objects defining the MLP architecture.\n",
    "            loss_method (Layer): A loss function layer used to compute errors and gradients.\n",
    "            lr (float, optional): The learning rate used for weight updates. Default is 0.01.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.loss_method = loss_method\n",
    "        self.lr = lr\n",
    "        self.outputs = None\n",
    "\n",
    "    def forward(self, input_: np.ndarray):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the MLP network.\n",
    "\n",
    "        Args:\n",
    "            input_ (np.ndarray): The input data to the network.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The output from the network after the forward pass.\n",
    "        \"\"\"\n",
    "        self.outputs = input_\n",
    "        for layer in self.layers:\n",
    "            self.outputs = layer.forward(self.outputs)\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Performs backpropagation to compute gradients for all layers.\n",
    "        Starts from the loss function layer and propagates backwards through the network.\n",
    "        \"\"\"\n",
    "        upstream_grad = self.loss_method.backward() \n",
    "        for layer in self.layers[::-1]:\n",
    "            upstream_grad = layer.backward(upstream_grad)\n",
    "\n",
    "    def update_weigths(self):\n",
    "        \"\"\"\n",
    "        Updates the weights of all layers using the gradients calculated during backpropagation.\n",
    "\n",
    "        Uses the specified learning rate for the weight updates.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.step(self.lr)\n",
    "\n",
    "    def train(self, input_: np.ndarray, y: np.ndarray, epoches=100, batch_size=1):\n",
    "        \"\"\"\n",
    "        Trains the MLP on the provided data for the specified number of epochs.\n",
    "\n",
    "        Args:\n",
    "            input_ (np.ndarray): The input training data.\n",
    "            y (np.ndarray): The target labels.\n",
    "            epoches (int, optional): The number of epochs for training. Default is 100.\n",
    "            batch_size (int, optional): The batch size used for training. Default is 1.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of training losses for each epoch.\n",
    "        \"\"\"\n",
    "        random_shuffle = np.random.permutation(input_.shape[0])\n",
    "        shuffled_data_x = input_[random_shuffle]\n",
    "        shuffled_data_y = y[random_shuffle]\n",
    "        rows, columns = input_.shape\n",
    "\n",
    "        train_losses = []\n",
    "\n",
    "        for epoch in (pbar := trange(epoches)):\n",
    "            random_indices = np.random.choice(rows, size=batch_size)\n",
    "            batch_data_x = shuffled_data_x[random_indices]\n",
    "            batch_data_y = shuffled_data_y[random_indices]\n",
    "\n",
    "            predicts = self.forward(batch_data_x)\n",
    "            \n",
    "            train_loss = self.loss_method.forward(batch_data_y, predicts)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            self.backward()\n",
    "            self.update_weigths()\n",
    "\n",
    "        return train_losses\n",
    "\n",
    "    def predict(self, input_: np.ndarray):\n",
    "        \"\"\"\n",
    "        Makes predictions on new input data after training.\n",
    "\n",
    "        Args:\n",
    "            input_ (np.ndarray): The input data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The predicted output for the given input.\n",
    "        \"\"\"\n",
    "        return self.forward(input_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
