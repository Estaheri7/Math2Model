{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0a930a",
   "metadata": {},
   "source": [
    "# Latent ODE-VAE Implementation Using PyTorch\n",
    "\n",
    "This notebook demonstrates the implementation of a **Latent ODE Variational Autoencoder (Latent ODE-VAE)** from scratch using PyTorch. A Latent ODE-VAE combines Variational Autoencoders (VAEs) with Neural ODEs to model time-evolving latent variables in a continuous-time framework. This is especially useful for modeling temporal sequences like video frames or irregularly sampled time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60024ab3",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f5d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torchdiffeq import odeint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4d58ed",
   "metadata": {},
   "source": [
    "## Model Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd5d6ef",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "The `Encoder` maps input sequences to a latent distribution $q(\\mathbf{z}_0 \\mid \\mathbf{x}_{1:T})$, using stacked linear layers (or other architectures) to predict the mean and log-variance of the latent variable $\\mathbf{z}_0 \\in \\mathbb{R}^d$. Reparameterization is used to sample from this distribution.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "Let the input sequence be:\n",
    "$$\n",
    "\\mathbf{x}_{1:T} = [\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_T], \\quad \\mathbf{x}_t \\in \\mathbb{R}^D\n",
    "$$\n",
    "\n",
    "We define a recognition (inference) model that produces the posterior distribution over the initial latent state:\n",
    "$$\n",
    "q(\\mathbf{z}_0 \\mid \\mathbf{x}_{1:T}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{z}_0}, \\operatorname{diag}(\\boldsymbol{\\sigma}_{\\mathbf{z}_0}^2))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\boldsymbol{\\mu}_{\\mathbf{z}_0} \\in \\mathbb{R}^d$ is the mean of the approximate posterior.\n",
    "- $\\boldsymbol{\\sigma}_{\\mathbf{z}_0}^2 \\in \\mathbb{R}^d$ is the (diagonal) variance.\n",
    "- $d$ is the latent dimension.\n",
    "\n",
    "These parameters are learned via an encoder neural network, typically an RNN (e.g., GRU/LSTM) or a temporal convolutional model:\n",
    "$$\n",
    "[\\boldsymbol{\\mu}_{\\mathbf{z}_0}, \\log \\boldsymbol{\\sigma}_{\\mathbf{z}_0}^2] = \\text{Encoder}_{\\phi}(\\mathbf{x}_{1:T})\n",
    "$$\n",
    "\n",
    "#### Reparameterization Trick\n",
    "\n",
    "To make the model differentiable and enable backpropagation through the stochastic node $\\mathbf{z}_0$, we apply the **reparameterization trick**:\n",
    "$$\n",
    "\\mathbf{z}_0 = \\boldsymbol{\\mu}_{\\mathbf{z}_0} + \\boldsymbol{\\sigma}_{\\mathbf{z}_0} \\odot \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\odot$ is element-wise multiplication.\n",
    "- $\\boldsymbol{\\epsilon}$ is standard Gaussian noise.\n",
    "- Sampling is made differentiable via the transformation.\n",
    "\n",
    "#### Loss Term from Encoder (KL Divergence)\n",
    "\n",
    "The encoder also contributes to the ELBO loss via the KL divergence between the approximate posterior and a standard normal prior:\n",
    "$$\n",
    "\\mathrm{KL}\\left(q(\\mathbf{z}_0 \\mid \\mathbf{x}_{1:T}) \\, \\| \\, p(\\mathbf{z}_0)\\right) = \\frac{1}{2} \\sum_{i=1}^{d} \\left( \\sigma_{\\mathbf{z}_0, i}^2 + \\mu_{\\mathbf{z}_0, i}^2 - 1 - \\log \\sigma_{\\mathbf{z}_0, i}^2 \\right)\n",
    "$$\n",
    "\n",
    "This penalizes divergence from the standard normal prior $p(\\mathbf{z}_0) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ and encourages compact, regularized latent representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ba4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=784, latent_dim=32, rnn_hidden=128):\n",
    "        super().__init__()\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=64, hidden_size=rnn_hidden, batch_first=True, bidirectional=False)\n",
    "\n",
    "        self.to_mean = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden, rnn_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(rnn_hidden, latent_dim)\n",
    "        )\n",
    "\n",
    "        self.to_logvar = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden, rnn_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(rnn_hidden, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_seq): \n",
    "        x_seq = x_seq.flatten(2) # shape: (B, T, F)\n",
    "        B, T, D = x_seq.shape\n",
    "\n",
    "        # Step 1: extract features per time step\n",
    "        x_seq_flat = x_seq.view(B * T, D)\n",
    "        h_seq_flat = self.feature_net(x_seq_flat)  # → (B*T, 64)\n",
    "        h_seq = h_seq_flat.view(B, T, -1)  # (B, T, 64)\n",
    "\n",
    "        # Step 2: RNN over time\n",
    "        _, h_final = self.rnn(h_seq) # h_final: (1, B, rnn_hidden)\n",
    "        h_final = h_final.squeeze(0) # (B, rnn_hidden)\n",
    "\n",
    "        # Step 3: compute z₀ with reparameterization\n",
    "        mu = self.to_mean(h_final) # (B, latent_dim)\n",
    "        logvar = self.to_logvar(h_final)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        z0 = mu + eps * std  # reparameterization trick\n",
    "        \n",
    "        return z0, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b32388b",
   "metadata": {},
   "source": [
    "### ODE Function ($\\texttt{ODEFunc}$)\n",
    "\n",
    "The ODE function defines the continuous-time dynamics of the latent state $\\mathbf{z}(t)$ by modeling the derivative $\\frac{d\\mathbf{z}(t)}{dt}$ using a neural network.\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "To specify the system of differential equations that governs how the latent variable $\\mathbf{z}(t)$ evolves with respect to time.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "We assume that the latent state $\\mathbf{z}(t) \\in \\mathbb{R}^D$ evolves according to an ODE:\n",
    "$$\n",
    "\\frac{d\\mathbf{z}(t)}{dt} = f_\\theta(\\mathbf{z}(t), t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $f_\\theta : \\mathbb{R}^D \\times \\mathbb{R} \\rightarrow \\mathbb{R}^D$ is a neural network parameterized by $\\theta$.\n",
    "- $t$ is the time scalar.\n",
    "- The input to $f_\\theta$ is the concatenation of the latent state and the time: $\\mathbf{z}_t = [\\mathbf{z}(t); t] \\in \\mathbb{R}^{D+1}$.\n",
    "\n",
    "The neural network learns the vector field that defines the flow of the latent states in continuous time:\n",
    "$$\n",
    "f_\\theta(\\mathbf{z}(t), t) = \\text{MLP}([\\mathbf{z}(t); t])\n",
    "$$\n",
    "\n",
    "This function enables data-driven modeling of complex, non-linear dynamics over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af089e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 1, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, z):\n",
    "        # z: (batch_size, latent_dim), t: scalar\n",
    "        t_expand = torch.ones(z.shape[0], 1).to(z.device) * t # add time dimension\n",
    "        zt = torch.cat([z, t_expand], dim=1)\n",
    "        return self.net(zt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34f62cb",
   "metadata": {},
   "source": [
    "### Latent ODE Block ($\\texttt{LatentODEBlock}$)\n",
    "\n",
    "This component numerically integrates the ODE defined above, generating a full trajectory $\\mathbf{z}_{1:T}$ from an initial latent state $\\mathbf{z}_0$.\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "To solve the initial value problem (IVP):\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{d\\mathbf{z}(t)}{dt} = f_\\theta(\\mathbf{z}(t), t), \\\\\n",
    "\\mathbf{z}(t_0) = \\mathbf{z}_0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "over a discrete sequence of time points $\\{t_1, t_2, \\dots, t_T\\}$.\n",
    "\n",
    "#### Mathematical Operation\n",
    "\n",
    "We compute the latent trajectory:\n",
    "$$\n",
    "\\mathbf{z}_{1:T} = \\text{ODESolve}(\\mathbf{z}_0, f_\\theta, \\{t_1, \\dots, t_T\\})\n",
    "$$\n",
    "\n",
    "Where $\\text{ODESolve}$ denotes a numerical integration algorithm (e.g., Runge-Kutta 4). The output is a time-dependent trajectory:\n",
    "$$\n",
    "\\mathbf{z}_{1:T} = \\left[ \\mathbf{z}(t_1), \\mathbf{z}(t_2), \\dots, \\mathbf{z}(t_T) \\right]^\\top \\in \\mathbb{R}^{T \\times D}\n",
    "$$\n",
    "\n",
    "This step enables continuous-time generative modeling by evolving $\\mathbf{z}_0$ forward through learned dynamics.\n",
    "\n",
    "#### Gradient Flow\n",
    "\n",
    "Because modern ODE solvers used here are differentiable (via the adjoint sensitivity method or backpropagation through the solver), gradients can be computed with respect to both:\n",
    "- The initial condition $\\mathbf{z}_0$\n",
    "- The dynamics parameters $\\theta$ in $f_\\theta$\n",
    "\n",
    "Thus, the ODE solution is end-to-end trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72548bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentODEBlock(nn.Module):\n",
    "    def __init__(self, odefunc, solver='rk4'):\n",
    "        super().__init__()\n",
    "        self.odefunc = odefunc\n",
    "        self.solver = solver\n",
    "\n",
    "    def forward(self, z0, time_steps):  # z0: (B, D), time_steps: (T,)\n",
    "        zt = odeint(self.odefunc, z0, time_steps, method=self.solver)\n",
    "        return zt.permute(1, 0, 2)  # (T, B, D) → (B, T, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb16cc",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "The Decoder maps the latent trajectory $\\mathbf{z}_{1:T} \\in \\mathbb{R}^{T \\times D}$ back to the observation space, generating reconstructions $\\hat{\\mathbf{x}}_{1:T}$ of the input sequence $\\mathbf{x}_{1:T}$.\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "To model the likelihood $p_\\phi(\\mathbf{x}_{1:T} \\mid \\mathbf{z}_{1:T})$ using a neural network decoder, where $\\phi$ are the decoder parameters.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "Given the latent state at each time step $\\mathbf{z}(t_i) \\in \\mathbb{R}^D$, the decoder outputs a reconstruction $\\hat{\\mathbf{x}}(t_i)$:\n",
    "$$\n",
    "\\hat{\\mathbf{x}}(t_i) = g_\\phi(\\mathbf{z}(t_i)) \\in \\mathbb{R}^F\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $g_\\phi : \\mathbb{R}^D \\rightarrow \\mathbb{R}^F$ is a neural network (decoder).\n",
    "- $F$ is the dimensionality of each observation $\\mathbf{x}(t_i)$ (e.g., $784$ for MNIST).\n",
    "\n",
    "The entire reconstructed sequence is:\n",
    "$$\n",
    "\\hat{\\mathbf{x}}_{1:T} = \\left[ g_\\phi(\\mathbf{z}(t_1)), g_\\phi(\\mathbf{z}(t_2)), \\dots, g_\\phi(\\mathbf{z}(t_T)) \\right]^\\top \\in \\mathbb{R}^{T \\times F}\n",
    "$$\n",
    "\n",
    "#### Probabilistic Likelihood\n",
    "\n",
    "If we assume a Bernoulli observation model (e.g., for binarized MNIST), the likelihood of each pixel is modeled independently:\n",
    "$$\n",
    "p_\\phi(\\mathbf{x}(t) \\mid \\mathbf{z}(t)) = \\text{Bernoulli}(\\hat{\\mathbf{x}}(t))\n",
    "$$\n",
    "\n",
    "The reconstruction loss is then:\n",
    "$$\n",
    "\\log p_\\phi(\\mathbf{x}_{1:T} \\mid \\mathbf{z}_{1:T}) = \\sum_{t=1}^{T} \\sum_{i=1}^{F} \\left[ x_i(t) \\log \\hat{x}_i(t) + (1 - x_i(t)) \\log(1 - \\hat{x}_i(t)) \\right]\n",
    "$$\n",
    "\n",
    "#### Training Gradient Flow\n",
    "\n",
    "Since $g_\\phi$ is differentiable, gradients flow from the reconstruction loss through the decoder and back into the latent ODE solver and encoder. This enables the model to learn a trajectory $\\mathbf{z}_{1:T}$ that faithfully represents the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):  # z: (batch, time, latent_dim)\n",
    "        B, T, D = z.shape\n",
    "        z_flat = z.reshape(B * T, D) # (B*T, latent_dim)\n",
    "        x_flat = self.net(z_flat) # (B*T, 784)\n",
    "        x_seq = x_flat.reshape(B, T, 1, 28, 28)\n",
    "        return x_seq # (batch, time, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8b9448",
   "metadata": {},
   "source": [
    "### Evidence Lower Bound (ELBO) and Total Loss\n",
    "\n",
    "The training objective of the Latent ODE-VAE is based on **variational inference**, where we maximize the **Evidence Lower Bound (ELBO)** to approximate the intractable data likelihood $p(\\mathbf{x}_{1:T})$.\n",
    "\n",
    "#### 1. Variational Inference\n",
    "\n",
    "We approximate the true posterior $p(\\mathbf{z}_0 \\mid \\mathbf{x}_{1:T})$ with a variational distribution $q_\\psi(\\mathbf{z}_0 \\mid \\mathbf{x}_{1:T})$ parameterized by the encoder. The marginal likelihood is intractable:\n",
    "$$\n",
    "\\log p_\\theta(\\mathbf{x}_{1:T}) = \\log \\int p_\\theta(\\mathbf{x}_{1:T} \\mid \\mathbf{z}_0) p(\\mathbf{z}_0) d\\mathbf{z}_0\n",
    "$$\n",
    "\n",
    "Instead, we optimize the ELBO:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{ELBO}} = \\mathbb{E}_{q_\\psi(\\mathbf{z}_0 \\mid \\mathbf{x}_{1:T})} \\left[ \\log p_\\theta(\\mathbf{x}_{1:T} \\mid \\mathbf{z}_0) \\right] - \\text{KL}\\left(q_\\psi(\\mathbf{z}_0 \\mid \\mathbf{x}_{1:T}) \\| p(\\mathbf{z}_0)\\right)\n",
    "$$\n",
    "\n",
    "#### 2. Reconstruction Term\n",
    "\n",
    "The first term is the expected log-likelihood of the observed sequence given the latent trajectory:\n",
    "$$\n",
    "\\mathbb{E}_{q(\\mathbf{z}_0)} \\left[ \\log p_\\theta(\\mathbf{x}_{1:T} \\mid \\mathbf{z}_{1:T}) \\right] \\approx \\log p_\\theta(\\mathbf{x}_{1:T} \\mid \\mathbf{z}_{1:T})\n",
    "$$\n",
    "\n",
    "If we assume a Bernoulli observation model (common for binary MNIST), we use the **binary cross-entropy** loss:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{recon}} = - \\sum_{t=1}^T \\sum_{i=1}^F \\left[ x_i(t) \\log \\hat{x}_i(t) + (1 - x_i(t)) \\log(1 - \\hat{x}_i(t)) \\right]\n",
    "$$\n",
    "\n",
    "#### 3. KL Divergence\n",
    "\n",
    "The KL divergence term measures how much the approximate posterior $q(\\mathbf{z}_0)$ deviates from the prior $p(\\mathbf{z}_0) = \\mathcal{N}(0, I)$. For a Gaussian posterior:\n",
    "$$\n",
    "q(\\mathbf{z}_0) = \\mathcal{N}(\\mu, \\text{diag}(\\sigma^2)) \\quad \\text{where } \\log \\sigma^2 = \\text{logvar}\n",
    "$$\n",
    "\n",
    "The KL divergence becomes:\n",
    "$$\n",
    "\\text{KL}\\left(q(\\mathbf{z}_0) \\| p(\\mathbf{z}_0)\\right) = \\frac{1}{2} \\sum_{i=1}^D \\left( \\exp(\\log \\sigma_i^2) + \\mu_i^2 - 1 - \\log \\sigma_i^2 \\right)\n",
    "$$\n",
    "\n",
    "This penalizes the complexity of the approximate posterior, encouraging it to stay close to the prior.\n",
    "\n",
    "#### 4. Total Loss\n",
    "\n",
    "The total loss function minimized during training is the **negative ELBO**:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{recon}} + \\text{KL}\\left(q(\\mathbf{z}_0) \\| p(\\mathbf{z}_0)\\right)\n",
    "$$\n",
    "\n",
    "This balances:\n",
    "- Accurate reconstruction of the input sequence.\n",
    "- A smooth, compact latent space through regularization.\n",
    "\n",
    "#### Why ELBO?\n",
    "\n",
    "Directly maximizing the true log-likelihood is intractable. The ELBO provides a **tractable lower bound** that can be **optimized with stochastic gradient descent**, while still pushing the approximate posterior to explain the data and remain close to the prior.\n",
    "\n",
    "Thus, training with ELBO allows us to learn both:\n",
    "- A meaningful latent representation $\\mathbf{z}_0$\n",
    "- A generative model that can simulate plausible sequences from $\\mathbf{z}_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8df889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(mu, logvar):\n",
    "    kl = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return kl.sum(dim=1).mean()\n",
    "\n",
    "def compute_loss(x_true, x_recon, mu, logvar, beta=0.1):\n",
    "    recon = F.binary_cross_entropy(x_recon, x_true, reduction='sum') / x_true.shape[0]\n",
    "    kl = kl_divergence(mu, logvar)\n",
    "    return recon + beta * kl, recon, kl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9acf7c",
   "metadata": {},
   "source": [
    "## CustomRotatingMnist dataset\n",
    "\n",
    "Since we latent ode-vae works well with time series, in this part we create a time series mnist dataset called `RotatingMnist`.\n",
    "\n",
    "Each image is repeated T times with different rotation angles and stacked with shape (T, 1, 28, 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b21f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotatingMNISTDataset(Dataset):\n",
    "    def __init__(self, dataset_root='../../../datasets', train=True, angles=None):\n",
    "        self.angles = angles if angles is not None else [0, 72, 144, 216, 288]\n",
    "        self.mnist = datasets.MNIST(root=dataset_root, train=train, download=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist[idx]\n",
    "        rotated_sequence = []\n",
    "        \n",
    "        for angle in self.angles:\n",
    "            rotated = TF.rotate(image, angle)\n",
    "            rotated = TF.to_tensor(rotated)\n",
    "            rotated_sequence.append(rotated)\n",
    "        \n",
    "        # T depends on angles list\n",
    "        rotated_sequence = torch.stack(rotated_sequence, dim=0)  # Shape: (T, 1, 28, 28)\n",
    "        return rotated_sequence, label\n",
    "\n",
    "train_dataset = RotatingMNISTDataset(train=True)\n",
    "dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0d0fe",
   "metadata": {},
   "source": [
    "## Model Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e61c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_latent_ode(model_components, dataloader, optimizer, time_steps, device):\n",
    "    encoder, odeblock, decoder = model_components\n",
    "    encoder.train(), odeblock.train(), decoder.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for x in dataloader:\n",
    "        x = x[0].to(device)\n",
    "        z0, mu, logvar = encoder(x)\n",
    "        z_t = odeblock(z0, time_steps) # (B, T, latent_dim)\n",
    "        x_recon = decoder(z_t) # (B, T, 784)\n",
    "\n",
    "        loss, recon_loss, kl_loss = compute_loss(x, x_recon, mu, logvar, 1)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a3754",
   "metadata": {},
   "source": [
    "### Train model for 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done. with loss 974.6031277459314\n",
      "Epoch 1 done. with loss 689.1727114028768\n",
      "Epoch 2 done. with loss 610.413347516741\n",
      "Epoch 3 done. with loss 568.7308122516949\n",
      "Epoch 4 done. with loss 545.1143848931612\n",
      "Epoch 5 done. with loss 526.8455268843596\n",
      "Epoch 6 done. with loss 512.5183383966051\n",
      "Epoch 7 done. with loss 501.18272969260147\n",
      "Epoch 8 done. with loss 492.18749788524246\n",
      "Epoch 9 done. with loss 484.68065814117887\n",
      "Epoch 10 done. with loss 477.8773072004827\n",
      "Epoch 11 done. with loss 472.1536882477783\n",
      "Epoch 12 done. with loss 467.128277060828\n",
      "Epoch 13 done. with loss 462.5399680391812\n",
      "Epoch 14 done. with loss 458.3475142033624\n",
      "Epoch 15 done. with loss 454.76136116076634\n",
      "Epoch 16 done. with loss 451.70131258771363\n",
      "Epoch 17 done. with loss 448.65400393227776\n",
      "Epoch 18 done. with loss 445.8372091525145\n",
      "Epoch 19 done. with loss 443.39330316149096\n",
      "Epoch 20 done. with loss 440.94525478338636\n",
      "Epoch 21 done. with loss 438.8999432399075\n",
      "Epoch 22 done. with loss 436.89889575169286\n",
      "Epoch 23 done. with loss 435.08061751831315\n",
      "Epoch 24 done. with loss 433.2689265594808\n",
      "Epoch 25 done. with loss 431.6616876598106\n",
      "Epoch 26 done. with loss 430.128765342078\n",
      "Epoch 27 done. with loss 428.9054989682586\n",
      "Epoch 28 done. with loss 427.45968253780273\n",
      "Epoch 29 done. with loss 426.2819947200035\n",
      "Epoch 30 done. with loss 425.4455630174324\n",
      "Epoch 31 done. with loss 424.2115646394839\n",
      "Epoch 32 done. with loss 423.47514060209556\n",
      "Epoch 33 done. with loss 422.46984072687275\n",
      "Epoch 34 done. with loss 421.7582478797766\n",
      "Epoch 35 done. with loss 421.0394073030842\n",
      "Epoch 36 done. with loss 420.1190277294818\n",
      "Epoch 37 done. with loss 419.3941253466901\n",
      "Epoch 38 done. with loss 418.83917141977406\n",
      "Epoch 39 done. with loss 418.14951540818856\n",
      "Epoch 40 done. with loss 417.6906063185572\n",
      "Epoch 41 done. with loss 417.09519997537774\n",
      "Epoch 42 done. with loss 416.3938204962562\n",
      "Epoch 43 done. with loss 415.8697257621456\n",
      "Epoch 44 done. with loss 415.19559360008\n",
      "Epoch 45 done. with loss 414.86310842550637\n",
      "Epoch 46 done. with loss 414.4321569186538\n",
      "Epoch 47 done. with loss 414.0076520061696\n",
      "Epoch 48 done. with loss 413.4242732977308\n",
      "Epoch 49 done. with loss 412.95689883364287\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 32\n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "odefunc = ODEFunc(latent_dim)\n",
    "odeblock = LatentODEBlock(odefunc)\n",
    "decoder = Decoder(latent_dim)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder, decoder, odeblock = encoder.to(device), decoder.to(device), odeblock.to(device)\n",
    "\n",
    "params = list(encoder.parameters()) + list(odeblock.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.1e-2)\n",
    "\n",
    "T = 5\n",
    "time_steps = torch.linspace(0., 1., T).to(device)\n",
    "\n",
    "for epoch in range(50):\n",
    "    avg_loss = train_latent_ode((encoder, odeblock, decoder), dataloader, optimizer, time_steps, device)\n",
    "    print(f\"Epoch {epoch} done. with loss {avg_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8569895",
   "metadata": {},
   "source": [
    "## Generating samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c03ddf73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x100 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate_and_plot_samples(odeblock, decoder, latent_dim, time_steps, n_samples=4, device='cuda'):\n",
    "    # Sample z0 ~ N(0, I)\n",
    "    z0 = torch.randn(n_samples, latent_dim).to(device)\n",
    "    \n",
    "    # Integrate latent ODE (B, T, D)\n",
    "    zt = odeblock(z0, time_steps)\n",
    "    \n",
    "    # Decode: (B, T, 1, 28, 28)\n",
    "    x_recon = decoder(zt)\n",
    "\n",
    "    # Plot each sequence\n",
    "    fig, axes = plt.subplots(n_samples, len(time_steps), figsize=(len(time_steps), n_samples))\n",
    "    if n_samples == 1:\n",
    "        axes = axes[None, :]\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        for j in range(len(time_steps)):\n",
    "            axes[i, j].imshow(x_recon[i, j, 0].cpu(), cmap='gray')\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "generate_and_plot_samples(odeblock, decoder, latent_dim, time_steps=time_steps, n_samples=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GLOBAL",
   "language": "python",
   "name": "global"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
