{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Ensemble learning is a technique that combines multiple base models to produce a stronger overall model. The main idea is that by combining the predictions of several models, the ensemble model can achieve better performance and generalization than any individual model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.datasets import make_classification, make_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Both regression and classification RandomForest model (Bagging)\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "Random Forest is an ensemble learning method that operates by constructing a multitude of decision trees during training time and outputting either the **mode** of the classes (classification) or the **mean** of the predictions (regression) of the individual trees.\n",
    "\n",
    "It combines the concepts of **bagging** and **feature randomness** to build a collection of **de-correlated decision trees**, whose predictions are then aggregated.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts of Random Forest\n",
    "\n",
    "1. **Bootstrap Aggregation (Bagging)**:\n",
    "   - Each decision tree is trained on a different subset of the training data, sampled **with replacement**.\n",
    "   - This technique reduces **variance** and helps prevent **overfitting**.\n",
    "\n",
    "2. **Feature Subsampling**:\n",
    "   - At each split in a tree (or for each tree, depending on implementation), only a **random subset of features** is considered.\n",
    "   - For classification, it is common to use:  \n",
    "     $$ \\text{feature\\_size} = \\lfloor \\sqrt{d} \\rfloor $$\n",
    "   - For regression, a common choice is:  \n",
    "     $$ \\text{feature\\_size} = \\left\\lfloor \\frac{d}{3} \\right\\rfloor $$\n",
    "   - Where \\( d \\) is the number of features in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Random Forest Algorithm\n",
    "\n",
    "#### Training (fit)\n",
    "\n",
    "Given a dataset (X, y):\n",
    "\n",
    "For each of the T decision trees (i.e., the number of ensembles):\n",
    "\n",
    "1. **Random Sampling**:\n",
    "   - Select a random subset of samples from X, with replacement (bootstrap sample).\n",
    "   - Randomly select a subset of features (without replacement).\n",
    "\n",
    "2. **Training**:\n",
    "   - Train a decision tree (classification or regression) on the sampled data and features.\n",
    "\n",
    "The result is a collection of models:\n",
    "$$ \\{ (h_1, F_1), (h_2, F_2), \\dots, (h_T, F_T) \\} $$\n",
    "Where:\n",
    "- $h_i$ is the trained tree,\n",
    "- $F_i \\subset \\{1, \\dots, d\\}$ is the subset of features used by that tree.\n",
    "\n",
    "---\n",
    "\n",
    "#### Prediction (predict)\n",
    "\n",
    "To make a prediction for a new data point $\\mathbf{x}$:\n",
    "\n",
    "- For each trained tree $h_i$, predict using only the features in $F_i$.\n",
    "- For classification:\n",
    "  $$ \\hat{y} = \\text{mode}(\\{ h_1(\\mathbf{x}_{F_1}), \\dots, h_T(\\mathbf{x}_{F_T}) \\}) $$\n",
    "- For regression:\n",
    "  $$ \\hat{y} = \\frac{1}{T} \\sum_{i=1}^{T} h_i(\\mathbf{x}_{F_i}) $$\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages of Random Forest\n",
    "\n",
    "- Handles both classification and regression tasks.\n",
    "- Works well with both numerical and categorical data.\n",
    "- Reduces overfitting compared to individual decision trees.\n",
    "- Can handle large datasets efficiently.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Less interpretable compared to a single decision tree.\n",
    "- More computationally expensive (especially with many trees or high-dimensional data).\n",
    "- Can be biased towards dominant classes in imbalanced classification tasks unless sampling is adjusted.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    A RandomForest implementation for either classification or regression tasks.\n",
    "    \n",
    "    Attributes:\n",
    "        n_ensembles (int): The number of decision trees in the forest.\n",
    "        weak_learner (class): The type of weak learner, which is a decision tree.\n",
    "        learner_type (str): Type of model ('classification' or 'regression').\n",
    "        feature_size (int): Number of features to consider when building each tree.\n",
    "        models (list): List of trained decision trees.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_ensembles=100, learner_type='classification'):\n",
    "        \"\"\"\n",
    "        Initializes the RandomForest with the specified number of ensembles and learner type.\n",
    "        \n",
    "        Parameters:\n",
    "            n_ensembles (int): The number of decision trees in the forest. Default is 100.\n",
    "            learner_type (str): Type of model ('classification' or 'regression'). Default is 'classification'.\n",
    "        \"\"\"\n",
    "        self.n_ensembles = n_ensembles\n",
    "        self.weak_learner = None\n",
    "        self.learner_type = learner_type\n",
    "        self.feature_size = 0\n",
    "        self.models = []\n",
    "        self.set_weak_learner()\n",
    "\n",
    "    def set_weak_learner(self):\n",
    "        \"\"\"\n",
    "        Sets the weak learner to be used for training the forest.\n",
    "        If the learner type is 'classification', uses DecisionTreeClassifier.\n",
    "        If the learner type is 'regression', uses DecisionTreeRegressor.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If the learner type is neither 'classification' nor 'regression'.\n",
    "        \"\"\"\n",
    "        if self.learner_type == 'classification':\n",
    "            self.weak_learner = DecisionTreeClassifier\n",
    "        elif self.learner_type == 'regression':\n",
    "            self.weak_learner = DecisionTreeRegressor\n",
    "        else:\n",
    "            raise ValueError('Invalid learner type, use \"classification\" or \"regression\"')\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Trains the RandomForest using bootstrapping and feature sampling for each ensemble.\n",
    "        \n",
    "        Parameters:\n",
    "            X (np.ndarray): The input feature matrix.\n",
    "            y (np.ndarray): The target values.\n",
    "        \n",
    "        The method trains `n_ensembles` number of weak learners (decision trees), each trained on\n",
    "        a bootstrapped sample of the data with a random subset of features.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        if self.learner_type == 'classification':\n",
    "            self.feature_size = np.floor(np.sqrt(n_features)).astype(int)\n",
    "        elif self.learner_type == 'regression':\n",
    "            self.feature_size = max(1, n_features // 3)\n",
    "\n",
    "        for _ in range(self.n_ensembles):\n",
    "            random_features = np.random.choice(n_features, size=self.feature_size, replace=False)\n",
    "            bootstrapped_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            X_data, y_data = X[bootstrapped_indices, :][:, random_features], y[bootstrapped_indices]\n",
    "\n",
    "            model = self.weak_learner()\n",
    "            model.fit(X_data, y_data)\n",
    "            self.models.append((model, random_features))\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Predicts the output by aggregating the predictions of all weak learners (decision trees).\n",
    "        \n",
    "        Parameters:\n",
    "            X (np.ndarray): The input feature matrix.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The predicted values for each sample.\n",
    "            \n",
    "        The method aggregates the predictions from all trees in the forest. For classification, it\n",
    "        returns the majority vote. For regression, it returns the mean of the predictions.\n",
    "        \"\"\"\n",
    "        weak_predicts = []\n",
    "        for model, features in self.models:\n",
    "            weak_predicts.append(model.predict(X[:, features]))\n",
    "\n",
    "        predicits = np.array(weak_predicts)\n",
    "        if self.learner_type == 'classification':\n",
    "            return mode(predicits, axis=0).mode[0]\n",
    "        return np.mean(predicits, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boostings\n",
    "\n",
    "## AdaBoost (Adaptive Boosting)\n",
    "\n",
    "AdaBoost, short for **Adaptive Boosting**, is an ensemble learning technique that combines multiple **weak learners** to create a **strong classifier**. It works by training weak models sequentially, each one correcting the errors of the previous one through **reweighting** the training samples.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition Behind AdaBoost\n",
    "\n",
    "- Emphasizes difficult samples that previous models misclassified.\n",
    "- Reduces weight of samples correctly classified.\n",
    "- Combines learners using weighted majority vote (for classification).\n",
    "\n",
    "---\n",
    "\n",
    "### Workflow of AdaBoost\n",
    "\n",
    "#### Step-by-Step\n",
    "\n",
    "Given a training set (X, y), where $y \\in \\{-1, +1\\}$:\n",
    "\n",
    "1. **Initialize sample weights**:  \n",
    "   Each data point gets an equal weight:  \n",
    "   $$ w_i^{(1)} = \\frac{1}{N} $$\n",
    "\n",
    "2. **Iterate for each weak learner** (total of \\( M \\) estimators):\n",
    "\n",
    "   - Train a weak classifier $h_m(x)$ using the current sample weights $ w_i^{(m)} $.\n",
    "   - Compute weighted error:\n",
    "     $$\n",
    "     \\epsilon_m = \\sum_{i=1}^{N} w_i^{(m)} \\cdot \\mathbb{1}(h_m(x_i) \\ne y_i)\n",
    "     $$\n",
    "   - Compute learner weight (confidence):\n",
    "     $$\n",
    "     \\alpha_m = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_m}{\\epsilon_m}\\right)\n",
    "     $$\n",
    "   - Update sample weights:\n",
    "     $$\n",
    "     w_i^{(m+1)} = w_i^{(m)} \\cdot \\exp(-\\alpha_m y_i h_m(x_i))\n",
    "     $$\n",
    "   - Normalize weights so they sum to 1.\n",
    "\n",
    "3. **Final prediction** is a **weighted majority vote**:\n",
    "   $$\n",
    "   H(x) = \\text{sign}\\left( \\sum_{m=1}^{M} \\alpha_m h_m(x) \\right)\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Key Components in Code\n",
    "\n",
    "- `sample_weight`: Gives importance to each sample.\n",
    "- `error_m`: Measures how well a weak model performed.\n",
    "- `alpha`: Influence of each model in the final prediction.\n",
    "- `np.sign`: Final decision using majority vote weighted by alpha.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages of AdaBoost\n",
    "\n",
    "- Often improves accuracy of weak learners.\n",
    "- Less prone to overfitting than other ensemble methods.\n",
    "- Works well even when weak learners are very simple (e.g., decision stumps).\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Sensitive to noisy data and outliers (due to exponential weighting).\n",
    "- Slower than bagging-based models (due to sequential nature).\n",
    "- Requires base learner that supports weighted samples (like `DecisionTreeClassifier` with `sample_weight`).\n",
    "\n",
    "---\n",
    "\n",
    "### Visual Overview\n",
    "\n",
    "1. Initialize equal weights for all data points.  \n",
    "2. Train a weak learner.  \n",
    "3. Increase weights for misclassified samples.  \n",
    "4. Add weak learner to ensemble with weight $\\alpha$.  \n",
    "5. Final prediction is sign of weighted sum of predictions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    \"\"\"\n",
    "    AdaBoost (Adaptive Boosting) implementation for classification tasks.\n",
    "    \n",
    "    AdaBoost is an ensemble method that combines multiple weak learners (typically decision trees) \n",
    "    to create a strong classifier by focusing on the mistakes made by previous models.\n",
    "    \n",
    "    Attributes:\n",
    "        model_type (str): The type of base model, default is 'tree' (DecisionTreeClassifier).\n",
    "        model (class): The weak learner model class, either DecisionTreeClassifier or another specified model.\n",
    "        n_estimators (int): The number of base models to be trained, default is 100.\n",
    "        weights (np.ndarray): The sample weights used for boosting.\n",
    "        models (list): List of weak learners (trained models).\n",
    "        alphas (list): The model coefficients for each weak learner.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_type='tree', n_estimators=100):\n",
    "        \"\"\"\n",
    "        Initializes the AdaBoost ensemble model with the specified base model and number of estimators.\n",
    "        \n",
    "        Parameters:\n",
    "            model_type (str): The type of model to use for weak learners. Defaults to 'tree' (DecisionTreeClassifier).\n",
    "            n_estimators (int): The number of base learners to train. Defaults to 100.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.model = None\n",
    "        self.n_estimators = n_estimators\n",
    "        self.weights = None\n",
    "        self.models = []\n",
    "        self.alphas = []\n",
    "        self.set_model()\n",
    "\n",
    "    def set_model(self):\n",
    "        \"\"\"\n",
    "        Sets the weak learner (base model) for boosting based on the specified model type.\n",
    "        \n",
    "        If `model_type` is 'tree', sets the model as `DecisionTreeClassifier`.\n",
    "        \"\"\"\n",
    "        if self.model_type == 'tree':\n",
    "            self.model = DecisionTreeClassifier\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Trains the AdaBoost model using the specified data and labels.\n",
    "        \n",
    "        The method iteratively trains weak learners and adjusts the sample weights \n",
    "        to emphasize the misclassified samples, boosting the model's performance.\n",
    "        \n",
    "        Parameters:\n",
    "            X (np.ndarray): The input feature matrix.\n",
    "            y (np.ndarray): The target labels.\n",
    "        \n",
    "        The method updates the model weights and stores the weak learners and their associated alphas.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        self.weights = np.ones(n_samples) / n_samples\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            model = self.model()\n",
    "            model.fit(X, y, sample_weight=self.weights)\n",
    "\n",
    "            y_hat = model.predict(X)\n",
    "\n",
    "            j_w = np.sum(self.weights * (y_hat != y))\n",
    "            error_m = j_w\n",
    "\n",
    "            # Calculate model weight (alpha)\n",
    "            alpha = 0.5 * np.log((1 - error_m) / max(error_m, 1e-10))\n",
    "\n",
    "            # Update the sample weights\n",
    "            self.weights = self.weights * np.exp(-alpha * y_hat * y)\n",
    "            self.weights /= np.sum(self.weights)\n",
    "\n",
    "            # Store the model and its corresponding alpha value\n",
    "            self.models.append(model)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Makes predictions using the trained AdaBoost model by combining the predictions \n",
    "        of all weak learners weighted by their corresponding alphas.\n",
    "        \n",
    "        Parameters:\n",
    "            X (np.ndarray): The input feature matrix.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The final predicted labels, obtained by taking the weighted majority vote.\n",
    "        \"\"\"\n",
    "        predicts = np.array([model.predict(X) for model in self.models])\n",
    "        return np.sign(np.array(self.alphas) @ predicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
