{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Linear Regression is a fundamental and widely used algorithm in machine learning and statistics. Its goal is to model the relationship between a `dependent variable y` and one or more `independent variables x`, assuming this relationship is linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Hypothesis Function and Loss\n",
    "\n",
    "### Hypothesis Function\n",
    "In Linear Regression, the hypothesis function is used to predict the output (dependent variable) based on the input features (independent variables). The hypothesis function is defined as:\n",
    "\n",
    "$$ \\hat{y} = \\mathbf{w}^T\\mathbf{x} + \\mathbf{\\beta}$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y} $ is the predicted value.\n",
    "- $\\beta$ is the bias term (intercept).\n",
    "- $\\mathbf{w}$ are the coefficients (weights) for the input features.\n",
    "- $\\mathbf{x}$ are the input features.\n",
    "\n",
    "In matrix form, the hypothesis function can be written as:\n",
    "\n",
    "$$ \\mathbf{y} = \\mathbf{X}\\mathbf{w} $$\n",
    "\n",
    "Where `X` is the input matrix with a bias term added, and `w` is the vector of weights.\n",
    "\n",
    "### Loss Function\n",
    "The loss function in Linear Regression is used to measure the error between the predicted values and the actual target values. The most commonly used loss function is the Mean Squared Error (MSE), which is defined as:\n",
    "\n",
    "$$ \\mathcal{L}(\\mathbf{w}) = \\frac{1}{N} \\| X\\mathbf{w} - \\mathbf{y} \\|^2$$\n",
    "\n",
    "Where:\n",
    "- N is the number of training samples.\n",
    "\n",
    "The goal of Linear Regression is to minimize the loss function $\\mathcal{L}$ by finding the optimal values of $ \\mathbf{w} $. This can be achieved using optimization techniques such as Gradient Descent or the Normal Equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Regression Model\n",
    "\n",
    "A base class for regression models.\n",
    "\n",
    "This class provides a template for implementing regression models. \n",
    "It includes methods for adding a bias term to the input data, \n",
    "calculating the R-squared score, and placeholders for fitting \n",
    "and predicting, which should be implemented by subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel:\n",
    "    \"\"\"\n",
    "    A base class for regression models.\n",
    "\n",
    "    This class provides a template for implementing regression models. \n",
    "    It includes methods for adding a bias term to the input data, \n",
    "    calculating the R-squared score, and placeholders for fitting \n",
    "    and predicting, which should be implemented by subclasses.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the RegressionModel with default attributes for coefficients, bias, and weights.\n",
    "        \"\"\"\n",
    "        self.coef = None\n",
    "        self.bias = None\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Placeholder method for fitting the model to the data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (np.ndarray): The input features.\n",
    "        - y (np.ndarray): The target values.\n",
    "\n",
    "        Raises:\n",
    "        - NotImplementedError: This method should be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Subclasses should implement this class')\n",
    "    \n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Placeholder method for making predictions.\n",
    "\n",
    "        Parameters:\n",
    "        - X (np.ndarray): The input features.\n",
    "\n",
    "        Returns:\n",
    "        - None: This method should be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def add_bias(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Adds a bias term (column of ones) to the input features.\n",
    "\n",
    "        Parameters:\n",
    "        - X (np.ndarray): The input features.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The input features with an added bias term.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        bias_term = np.ones((n_samples, 1))\n",
    "        X_bias = np.hstack((bias_term, X))\n",
    "\n",
    "        return X_bias\n",
    "    \n",
    "    def score(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Calculates the R-squared score of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - X (np.ndarray): The input features.\n",
    "        - y (np.ndarray): The true target values.\n",
    "\n",
    "        Returns:\n",
    "        - float: The R-squared score, a measure of how well the model explains the variance in the data.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        ss_total = np.sum((y - np.mean(y)) ** 2)\n",
    "        ss_residual = np.sum((y - predictions) ** 2)\n",
    "        return 1 - (ss_residual / ss_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Regression model trained by normal Equation\n",
    "\n",
    "One of techniques for optimizing the loss function is to solve equation:\n",
    "$$ \\frac{\\partial{\\mathcal{L}}}{\\partial{\\mathbf{w}}} = 0$$\n",
    "\n",
    "which is:\n",
    "$$ \\frac{\\partial{\\mathcal{L}}}{\\partial{\\mathbf{w}}} = \\frac{1}{N}\\mathbf{X}^T(\\mathbf{X}\\mathbf{w} - \\mathbf{y}) = 0$$\n",
    "\n",
    "this results:\n",
    "$$ \\boxed{\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}}$$\n",
    "\n",
    "That is, normal equation can find the optimal weights for the loss function.\n",
    "\n",
    "### Limitations of Using the Normal Equation for Linear Regression\n",
    "\n",
    "1. **Computational Complexity (O(dÂ³))**:\n",
    "   - The normal equation requires computing $\\mathbf{X}^T \\mathbf{X}$  and then inverting the resulting matrix, which has a computational cost of $O(d^3)$, where `d` is the number of features (dimensionality of the feature vector).\n",
    "   - This becomes inefficient for **high-dimensional** datasets, as the matrix inversion grows computationally expensive. For very large `d`, this may become impractical or very slow.\n",
    "\n",
    "2. **Singular Matrix Issue**:\n",
    "   - If $\\mathbf{X}^T \\mathbf{X}$ is **singular** or **non-invertible** (i.e., its determinant is zero), the normal equation cannot be solved. This typically happens when:\n",
    "     - **Multicollinearity** exists in the dataset (i.e., some features are linearly dependent or highly correlated).\n",
    "     - **Insufficient data** relative to the number of features.\n",
    "   - In these cases, regularization techniques or dimensionality reduction methods are needed to ensure invertibility.\n",
    "\n",
    "3. **Memory Usage**:\n",
    "   - For very large datasets with many features `d`, storing and manipulating the matrices $\\mathbf{X}^T \\mathbf{X}$ can require substantial memory, which may not be feasible in low-memory environments.\n",
    "\n",
    "4. **Doesn't Scale Well with Large Datasets**:\n",
    "   - For large datasets with a large number of training examples `N`, the **gradient descent** method might be more efficient because it updates weights iteratively, rather than requiring the entire matrix to be inverted. This makes **gradient descent** better suited for big data.\n",
    "\n",
    "5. **No Flexibility for Regularization**:\n",
    "   - The normal equation by itself doesn't incorporate **regularization** (e.g., L2 or L1 regularization), which is often necessary to prevent overfitting in high-dimensional problems. In contrast, gradient descent can be easily modified to include regularization terms like Ridge regression or Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalRegression(RegressionModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X = self.add_bias(X)\n",
    "\n",
    "        self.w = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.add_bias(X)\n",
    "        return X @ self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with gradient descend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Regression model trained by Gradient Descent\n",
    "\n",
    "Instead of solving the above gradient equation analytically, we can take **iterative steps** in the opposite direction of the gradient:\n",
    "\n",
    "Given the same loss function:\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}) = \\frac{1}{N} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|^2\n",
    "$$\n",
    "\n",
    "The gradient is:\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\mathcal{L} = \\frac{1}{N} \\mathbf{X}^T(\\mathbf{X}\\mathbf{w} - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "Then the **gradient descent update rule** is:\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\cdot \\frac{1}{N} \\mathbf{X}^T (\\mathbf{X}\\mathbf{w}^{(t)} - \\mathbf{y})\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Variants for Linear Regression\n",
    "\n",
    "There are different types of **Gradient Descent** methods, each with its own advantages and trade-offs. Here's an overview of the **Batch**, **Stochastic**, and **Mini-Batch** Gradient Descent algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Batch Gradient Descent**\n",
    "\n",
    "**Batch Gradient Descent** computes the gradient using the entire dataset in each iteration. This is a deterministic approach where the weights are updated only after computing the gradient for all samples.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Stable convergence.\n",
    "  - The model is updated after considering the entire dataset.\n",
    "\n",
    "- **Disadvantages:**\n",
    "  - Very slow for large datasets, as it needs to process the entire dataset for each update.\n",
    "  - Can consume a lot of memory.\n",
    "\n",
    "---\n",
    "\n",
    "In this method:\n",
    "- $\\mathbf{w}$ are the model parameters (weights).\n",
    "- $X$ is the feature matrix, and $y$ is the target values.\n",
    "- The gradient is computed using the entire dataset, and the weights are updated after each epoch.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)** computes the gradient using only a single randomly selected sample at each iteration. This makes the updates faster, but they can be noisy and lead to fluctuations in the loss curve.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Very fast since it updates the weights after every single sample.\n",
    "  - Works well for large datasets and online learning.\n",
    "\n",
    "- **Disadvantages:**\n",
    "  - Noisy updates can lead to unstable convergence.\n",
    "  - Requires a larger number of epochs to converge to the optimal solution.\n",
    "\n",
    "\n",
    "In this method:\n",
    "- A **random index** is selected, and the gradient is computed based on that single sample.\n",
    "- The weight update happens after every data point, which speeds up the process but can lead to noisy convergence.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Mini-Batch Gradient Descent**\n",
    "\n",
    "**Mini-Batch Gradient Descent** is a hybrid approach that combines elements of both Batch and Stochastic Gradient Descent. It divides the dataset into small batches and updates the weights based on each mini-batch. This method provides a balance between the efficiency of SGD and the stability of Batch Gradient Descent.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Faster than Batch Gradient Descent for large datasets.\n",
    "  - Can lead to more stable convergence than SGD.\n",
    "  - Suitable for parallel processing (especially in deep learning).\n",
    "\n",
    "- **Disadvantages:**\n",
    "  - Still requires tuning the batch size and learning rate.\n",
    "  - Can still be noisy if the batch size is too small.\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "In this method:\n",
    "- The dataset is randomly divided into **mini-batches**.\n",
    "- The weight update happens based on the average gradient computed from a subset of the data, providing a good trade-off between speed and stability.\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison of Gradient Descent Variants\n",
    "\n",
    "| **Method**            | **Computation Per Update**            | **Speed**  | **Convergence** | **Memory Usage**  |\n",
    "|-----------------------|---------------------------------------|------------|-----------------|-------------------|\n",
    "| **Batch Gradient Descent**   | Entire dataset                       | Slow       | Stable, deterministic | High              |\n",
    "| **Stochastic Gradient Descent (SGD)** | Single sample                     | Fast       | Noisy, fluctuates | Low               |\n",
    "| **Mini-Batch Gradient Descent** | Small batch                        | Medium     | More stable than SGD | Medium            |\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **Batch Gradient Descent** is ideal for smaller datasets and when you want precise updates.\n",
    "- **Stochastic Gradient Descent** is better for large datasets and online learning, but it requires careful tuning to reduce noise.\n",
    "- **Mini-Batch Gradient Descent** strikes a good balance and is widely used in practice, especially in machine learning and deep learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(RegressionModel):\n",
    "    \"\"\"\n",
    "    A Linear Regression model that supports Batch, Stochastic, and Mini-Batch Gradient Descent.\n",
    "\n",
    "    This class extends the `RegressionModel` base class and implements the Linear Regression algorithm\n",
    "    with support for different gradient descent variants. It includes methods for training the model\n",
    "    using Batch, Stochastic, or Mini-Batch Gradient Descent, and for making predictions.\n",
    "\n",
    "    Attributes:\n",
    "        epoches (int): The number of iterations for training the model.\n",
    "        learning_rate (float): The step size for gradient descent updates.\n",
    "        n_samples (int): The number of training samples in the dataset.\n",
    "        type (str): The type of gradient descent to use ('batch', 'stochastic', or 'mini-batch').\n",
    "        batch_size (int or None): The size of mini-batches for Mini-Batch Gradient Descent. If None, defaults to 10% of the dataset size.\n",
    "\n",
    "    Methods:\n",
    "        fit(X: np.ndarray, y: np.ndarray):\n",
    "            Trains the model using the specified gradient descent type.\n",
    "\n",
    "        fit_batch(X: np.ndarray, y: np.ndarray):\n",
    "            Trains the model using Batch Gradient Descent.\n",
    "\n",
    "        fit_stochastic(X: np.ndarray, y: np.ndarray):\n",
    "            Trains the model using Stochastic Gradient Descent.\n",
    "\n",
    "        fit_mini_batch(X: np.ndarray, y: np.ndarray):\n",
    "            Trains the model using Mini-Batch Gradient Descent.\n",
    "\n",
    "        predict(X: np.ndarray) -> np.ndarray:\n",
    "            Makes predictions for the given input features.\n",
    "    \"\"\"\n",
    "    def __init__(self, epoches=1000, learning_rate=0.1, lr_type='batch', batch_size=None):\n",
    "        \"\"\"\n",
    "        Initializes the LinearRegression model with the specified parameters.\n",
    "\n",
    "        Parameters:\n",
    "            epoches (int): The number of iterations for training the model. Default is 1000.\n",
    "            learning_rate (float): The step size for gradient descent updates. Default is 0.1.\n",
    "            lr_type (str): The type of gradient descent to use ('batch', 'stochastic', or 'mini-batch'). Default is 'batch'.\n",
    "            batch_size (int or None): The size of mini-batches for Mini-Batch Gradient Descent. Default is None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.epoches = epoches\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_samples = 0\n",
    "        self.type = lr_type\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Trains the model using the specified gradient descent type.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The input features.\n",
    "            y (np.ndarray): The target values.\n",
    "        \"\"\"\n",
    "        self.n_samples, n_features = X.shape[0], X.shape[1]\n",
    "        self.w = np.random.randn(n_features + 1) * 0.01\n",
    "        X_bias = self.add_bias(X)\n",
    "\n",
    "        if self.type == 'batch':\n",
    "            self.fit_batch(X_bias, y)\n",
    "        elif self.type == 'stochastic':\n",
    "            self.fit_stochastic(X_bias, y)\n",
    "        elif self.type == 'mini-batch':\n",
    "            self.fit_mini_batch(X_bias, y)\n",
    "\n",
    "    def fit_batch(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Trains the model using Batch Gradient Descent.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The input features with a bias term added.\n",
    "            y (np.ndarray): The target values.\n",
    "        \"\"\"\n",
    "        for _ in range(self.epoches):\n",
    "            predictions = X @ self.w\n",
    "            errors = predictions - y\n",
    "            gradient = (X.T @ errors) / self.n_samples\n",
    "\n",
    "            self.w = self.w - self.learning_rate * gradient\n",
    "\n",
    "    def fit_stochastic(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Trains the model using Stochastic Gradient Descent.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The input features with a bias term added.\n",
    "            y (np.ndarray): The target values.\n",
    "        \"\"\"\n",
    "        for _ in range(self.epoches):\n",
    "            random_idx = np.random.randint(self.n_samples)\n",
    "            random_sample = X[random_idx]\n",
    "            predict = self.w.T.dot(random_sample)\n",
    "            error = predict - y[random_idx]\n",
    "\n",
    "            stoch = error * random_sample\n",
    "            self.w = self.w - self.learning_rate * stoch\n",
    "\n",
    "    def fit_mini_batch(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Trains the model using Mini-Batch Gradient Descent.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The input features with a bias term added.\n",
    "            y (np.ndarray): The target values.\n",
    "        \"\"\"\n",
    "        for _ in range(self.epoches):\n",
    "            size = self.batch_size if self.batch_size else max(1, int(self.n_samples * 0.1))\n",
    "            random_inds = np.random.randint(0, self.n_samples, size=size)\n",
    "\n",
    "            batch_samples, batch_labels = X[random_inds], y[random_inds]\n",
    "            predictions = batch_samples @ self.w\n",
    "            errors = predictions - batch_labels\n",
    "\n",
    "            gradient = (batch_samples.T @ errors) / size\n",
    "\n",
    "            self.w = self.w - self.learning_rate * gradient\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Makes predictions for the given input features.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The input features.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The predicted values.\n",
    "        \"\"\"\n",
    "        X = self.add_bias(X)\n",
    "        return X @ self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Regularizations\n",
    "\n",
    "## Why Use Regularization?\n",
    "\n",
    "In linear regression, the goal is to fit a model that minimizes the residual sum of squares between the observed targets in the dataset and the targets predicted by the linear approximation. However, in many real-world problems, this approach can lead to **overfitting**, where the model learns the noise or random fluctuations in the data instead of the actual underlying patterns. This typically happens when we have a large number of features or when the model is too complex for the available data.\n",
    "\n",
    "### Overfitting\n",
    "Overfitting occurs when the model is too \"flexible\" and captures not just the true relationships but also the noise in the data. This can result in a model that performs well on the training data but poorly on unseen (test) data, as it has essentially \"memorized\" the training data rather than learning generalizable patterns.\n",
    "\n",
    "### Regularization\n",
    "Regularization is a technique used to reduce overfitting by adding a penalty term to the loss function. This penalty term discourages the model from fitting the noise by shrinking the magnitude of the coefficients. In this way, the model is forced to find a balance between fitting the training data and keeping the model as simple as possible.\n",
    "\n",
    "By introducing a regularization term, we add a cost to the modelâs complexity, typically by penalizing large weights. This helps in reducing the model's variance and improving its ability to generalize to new data.\n",
    "\n",
    "## Types of Regularization\n",
    "\n",
    "There are primarily two types of regularization techniques used in linear regression:\n",
    "\n",
    "### 1. **L2 Regularization (Ridge Regression)**\n",
    "\n",
    "Ridge regression adds a penalty proportional to the sum of the squares of the coefficients. The loss function becomes:\n",
    "\n",
    "$$ \\mathcal{L}(\\mathbf{w}) = \\frac{1}{N} \\| X\\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 $$\n",
    "\n",
    "Where:\n",
    "- $\\lambda$ is the regularization strength (also called the Ridge parameter).\n",
    "- $\\mathbf{w}$ is the vector of coefficients.\n",
    "\n",
    "This penalty shrinks the coefficients by forcing them to be small, but it does not set them exactly to zero.\n",
    "\n",
    "### 2. **L1 Regularization (Lasso Regression)**\n",
    "\n",
    "Lasso regression adds a penalty proportional to the sum of the absolute values of the coefficients. The loss function becomes:\n",
    "\n",
    "$$ \\mathcal{L}(\\mathbf{w}) = \\frac{1}{N} \\| X\\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|_1 $$\n",
    "\n",
    "Where:\n",
    "- $\\lambda$ is the regularization strength (also called the Lasso parameter).\n",
    "- $\\mathbf{w}$ is the vector of coefficients.\n",
    "\n",
    "Lasso has the added benefit of performing **feature selection**, as it can drive some of the coefficients to exactly zero.\n",
    "\n",
    "### 3. **Elastic Net Regularization**\n",
    "\n",
    "Elastic Net combines both L1 and L2 penalties. The loss function becomes:\n",
    "\n",
    "$$ \\mathcal{L}(\\mathbf{w}) = \\frac{1}{N} \\| X\\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\left( \\alpha \\| \\mathbf{w} \\|_1 + \\frac{1-\\alpha}{2} \\| \\mathbf{w} \\|_2^2 \\right) $$\n",
    "\n",
    "Where:\n",
    "- $\\lambda$ is the regularization strength.\n",
    "- $\\alpha$ controls the mix between Lasso $(\\alpha = 1)$ and Ridge $(\\alpha = 0)$ regularization.\n",
    "- $\\mathbf{w}$ is the vector of coefficients.\n",
    "\n",
    "Elastic Net is useful when there are many correlated features, as it tends to select one feature from a group of correlated features, unlike Lasso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedRegression(RegressionModel):\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, alpha=0.5, l=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Regularized Regression model.\n",
    "\n",
    "        Parameters:\n",
    "        - learning_rate: The learning rate for gradient descent.\n",
    "        - epochs: The number of iterations for gradient descent.\n",
    "        - alpha: The mixing parameter between L1 (Lasso) and L2 (Ridge) regularization.\n",
    "          alpha = 1 corresponds to Lasso, and alpha = 0 corresponds to Ridge.\n",
    "        - l: The regularization strength (lambda).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.l = l\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the model using gradient descent.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Feature matrix (n_samples x n_features).\n",
    "        - y: Target vector (n_samples).\n",
    "        \"\"\"\n",
    "        X_bias = self.add_bias(X)\n",
    "        self.n_samples, self.n_features = X_bias.shape\n",
    "        self.w = np.zeros(self.n_features)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            predictions = X_bias @ self.w\n",
    "            errors = predictions - y\n",
    "\n",
    "            gradient = (X_bias.T @ errors) / self.n_samples\n",
    "\n",
    "            l1_reg = self.alpha * self.l * np.sign(self.w)\n",
    "            l2_reg = (1 - self.alpha) * self.l * self.w\n",
    "\n",
    "            gradient += l1_reg + l2_reg\n",
    "\n",
    "            self.w -= self.learning_rate * gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Makes predictions on new data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Feature matrix (n_samples x n_features).\n",
    "        \n",
    "        Returns:\n",
    "        - np.ndarray: Predicted values for the input X.\n",
    "        \"\"\"\n",
    "        X_bias = self.add_bias(X)\n",
    "        return X_bias @ self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic Regression is a classification algorithm used to predict the probability of a binary outcome (i.e., two classes). Unlike Linear Regression, which is used for regression tasks, Logistic Regression is used for classification, where the output is a probability between 0 and 1, which can then be mapped to a class label.\n",
    "\n",
    "### Logistic Regression Hypothesis Function\n",
    "\n",
    "In Logistic Regression, we aim to predict the probability that an input sample belongs to a particular class (e.g., class 1). The hypothesis function is defined using the **sigmoid function**:\n",
    "\n",
    "$$ \\hat{y} = \\sigma(\\mathbf{w}^T \\mathbf{x} + \\mathbf{\\beta}) $$\n",
    "\n",
    "Where:\n",
    "- $ \\hat{y} $ is the predicted probability that the input belongs to class 1.\n",
    "- $ \\mathbf{w} $ is the vector of weights for the features.\n",
    "- $ \\mathbf{x} $ is the input feature vector.\n",
    "- $ \\mathbf{\\beta} $ is the bias term (intercept).\n",
    "- $ \\sigma(z) $ is the sigmoid function.\n",
    "\n",
    "In matrix form, the hypothesis function becomes:\n",
    "\n",
    "$$ \\hat{y} = \\sigma(\\mathbf{X} \\mathbf{w}) $$\n",
    "\n",
    "Where:\n",
    "- $ \\mathbf{X} $ is the feature matrix with the bias term added (typically a column of ones).\n",
    "- $ \\mathbf{w} $ is the vector of weights (including the bias term).\n",
    "- $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ is the **sigmoid function**.\n",
    "\n",
    "### Sigmoid Function\n",
    "\n",
    "The **sigmoid function** (also known as the logistic function) maps any real-valued number to a value between 0 and 1, making it suitable for modeling probabilities. The sigmoid function is defined as:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "Where:\n",
    "- $ z = \\mathbf{w}^T \\mathbf{x} $ is the linear combination of input features and weights (the \"logit\").\n",
    "- The output of the sigmoid function is a probability, which is used for binary classification.\n",
    "\n",
    "The sigmoid function has the following properties:\n",
    "- $ \\sigma(z) $ is continuous and smooth.\n",
    "- $ \\sigma(z) $ is strictly increasing.\n",
    "- $ \\sigma(z) $ outputs values between 0 and 1.\n",
    "\n",
    "### Logistic Regression Loss Function\n",
    "\n",
    "In logistic regression, we use the **log loss** (also called binary cross-entropy loss) to measure the difference between the predicted probabilities and the actual binary labels. The loss function for logistic regression is given by:\n",
    "\n",
    "$$ \\mathcal{L}(\\mathbf{w}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i}) \\right) $$\n",
    "\n",
    "Where:\n",
    "- $ N $ is the number of training samples.\n",
    "- $ y_i $ is the actual class label for the `i-th` sample (either 0 or 1).\n",
    "- $ \\hat{y_i} $ is the predicted probability for the `i-th` sample, computed using the sigmoid function $ \\sigma(\\mathbf{x_i} \\mathbf{w}) $.\n",
    "\n",
    "The goal of Logistic Regression is to **minimize** this loss function by adjusting the weights $ \\mathbf{w} $ using optimization techniques like **Gradient Descent**.\n",
    "\n",
    "### Gradient Descent for Logistic Regression\n",
    "\n",
    "To update the weights in logistic regression, we compute the gradient of the loss function with respect to the weights $ \\mathbf{w} $. The gradient is given by:\n",
    "\n",
    "$$ \\boxed{\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^{N} ( \\hat{y_i} - y_i ) \\mathbf{x_i}} $$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y_i} = \\sigma(\\mathbf{x_i} \\mathbf{w})$ is the predicted probability for the `i-th` sample.\n",
    "- $\\mathbf{x_i}$ is the feature vector for the `i-th` sample.\n",
    "\n",
    "This gradient can be used to update the weights using the **Gradient Descent** algorithm:\n",
    "\n",
    "$$ \\mathbf{w} = \\mathbf{w} - \\eta \\cdot \\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) $$\n",
    "\n",
    "Where:\n",
    "- $\\eta $ is the learning rate.\n",
    "- $\\mathbf{w}$ is the weight vector.\n",
    "\n",
    "The update rule involves iterating over the training data and adjusting the weights to minimize the loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(RegressionModel):\n",
    "    def __init__(self, epoches=1000, learning_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.epoches = epoches\n",
    "        self.learning_rate = learning_rate\n",
    "        self.probs = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def loss_function(self, X: np.ndarray, y: np.ndarray):\n",
    "        sigmoids = self.sigmoid(X @ self.w)\n",
    "\n",
    "        epsilon = 1e-15\n",
    "        # to be sure log(0) not happens\n",
    "        sigmoids = np.clip(sigmoids, epsilon, 1 - epsilon)\n",
    "        \n",
    "        loss = - np.mean(((y * np.log(sigmoids)) + ((1-y) * np.log(1 - sigmoids))))\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.w = np.zeros(X.shape[1]+1)\n",
    "        for _ in range(self.epoches):\n",
    "            predicts = self.predict(X)\n",
    "            X_bias = self.add_bias(X)\n",
    "            errors = predicts - y\n",
    "            \n",
    "            n_samples = X_bias.shape[0]\n",
    "            gradient = (X_bias.T @ errors) / n_samples\n",
    "\n",
    "            self.w = self.w - self.learning_rate * gradient\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        X = self.add_bias(X)\n",
    "        self.probs = self.sigmoid(X @ self.w)\n",
    "        return (self.probs >= 0.5).astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
