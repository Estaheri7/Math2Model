{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) - Mathematical Foundation\n",
    "\n",
    "## Objective\n",
    "PCA is a **dimensionality reduction** technique that transforms a dataset to a lower-dimensional space while preserving **as much variance as possible**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Mathematical Derivation\n",
    "\n",
    "Let:  \n",
    "- $X \\in \\mathbb{R}^{n \\times d}$ be the dataset, where:\n",
    "  - $n$ = number of samples  \n",
    "  - $d$ = number of features (dimensions)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Center the Data\n",
    "\n",
    "We subtract the mean of each feature to center the data:\n",
    "\n",
    "- Compute the mean:\n",
    "  $$\n",
    "  \\mu = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n",
    "  $$\n",
    "\n",
    "- Center the data:\n",
    "  $$\n",
    "  \\tilde{X} = X - \\mu\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Compute the Covariance Matrix\n",
    "\n",
    "The covariance matrix captures pairwise feature dependencies:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n - 1} \\tilde{X}^T \\tilde{X} \\in \\mathbb{R}^{d \\times d}\n",
    "$$\n",
    "\n",
    "Each entry $\\Sigma_{ij}$ represents the covariance between features $i$ and $j$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Eigen-Decomposition\n",
    "\n",
    "We compute the eigenvalues and eigenvectors of the covariance matrix:\n",
    "\n",
    "$$\n",
    "\\Sigma v_k = \\lambda_k v_k\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $v_k \\in \\mathbb{R}^{d}$ is the $k$-th eigenvector (principal axis)  \n",
    "- $\\lambda_k \\in \\mathbb{R}$ is the $k$-th eigenvalue (variance along $v_k$)\n",
    "\n",
    "The eigenvectors form an orthonormal basis:\n",
    "\n",
    "$$\n",
    "V = [v_1, v_2, \\dots, v_d] \\quad \\text{such that} \\quad V^T V = I\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Sort Eigenvalues and Select Top $k$\n",
    "\n",
    "Sort eigenvalues in decreasing order:\n",
    "\n",
    "$$\n",
    "\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_d\n",
    "$$\n",
    "\n",
    "Select the top $k$ eigenvectors to form the projection matrix:\n",
    "\n",
    "$$\n",
    "V_k = [v_1, v_2, \\dots, v_k] \\in \\mathbb{R}^{d \\times k}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Project the Data\n",
    "\n",
    "To get the reduced representation $Z \\in \\mathbb{R}^{n \\times k}$:\n",
    "\n",
    "$$\n",
    "Z = \\tilde{X} V_k\n",
    "$$\n",
    "\n",
    "Each row of $Z$ is a projection of the corresponding sample from the original space to the new $k$-dimensional space.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Goal**: Find a lower-dimensional subspace that captures the maximum variance in the data.\n",
    "- **Variance Maximization**: PCA chooses directions (principal components) where data varies the most.\n",
    "- **Uncorrelated Features**: The resulting components are orthogonal (statistically uncorrelated).\n",
    "- **Dimensionality Reduction**: We keep only the top $k$ eigenvectors to reduce from $d$ dimensions to $k$.\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "If your data is in $\\mathbb{R}^3$ and you apply PCA with $k = 2$, then:\n",
    "\n",
    "- The data is projected from 3D to 2D: $\\mathbb{R}^3 \\rightarrow \\mathbb{R}^2$\n",
    "- The 2D projection retains the directions of **maximum variance** in the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    \"\"\"\n",
    "    Principal Component Analysis (PCA) for dimensionality reduction.\n",
    "\n",
    "    This class implements PCA using eigen-decomposition of the covariance matrix.\n",
    "    It projects the data to a lower-dimensional subspace that captures the maximum variance.\n",
    "\n",
    "    Attributes:\n",
    "        k_features (int): Number of principal components (dimensions) to retain.\n",
    "        best_vectors (np.ndarray): Matrix of top-k eigenvectors (principal components) found during fitting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k_features):\n",
    "        \"\"\"\n",
    "        Initializes the PCA model.\n",
    "\n",
    "        Args:\n",
    "            k_features (int): The number of principal components to keep.\n",
    "        \"\"\"\n",
    "        self.k_features = k_features\n",
    "        self.best_vectors = None\n",
    "\n",
    "    def fit(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Fits the PCA model to the data by computing the top-k eigenvectors.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "                           Each row corresponds to a sample and each column to a feature.\n",
    "\n",
    "        Process:\n",
    "            - Centers the data by subtracting the mean.\n",
    "            - Computes the covariance matrix.\n",
    "            - Performs eigen-decomposition.\n",
    "            - Selects the top-k eigenvectors based on largest eigenvalues.\n",
    "        \"\"\"\n",
    "        X_meaned = X - X.mean(axis=0)\n",
    "        cov_matrix = np.cov(X_meaned, rowvar=False)\n",
    "        eighvalues, eighvectors = np.linalg.eigh(cov_matrix)\n",
    "        best_values = np.argsort(eighvalues)[::-1]\n",
    "        self.best_vectors = eighvectors[:, best_values][:, :self.k_features]\n",
    "\n",
    "    def transform(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Projects the data onto the top-k principal components.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Transformed data of shape (n_samples, k_features).\n",
    "        \"\"\"\n",
    "        return X @ self.best_vectors\n",
    "\n",
    "    def fit_transform(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Fits the model and transforms the data in one step.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Transformed data of shape (n_samples, k_features).\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
